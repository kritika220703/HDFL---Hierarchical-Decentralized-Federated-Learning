{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import pickle\n",
    "import time\n",
    "from torch.utils.data import ConcatDataset, Dataset, Subset\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CIFAR10('D:/downloads/btp/outputforpaper/CIFAR10', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = CIFAR10('D:/downloads/btp/outputforpaper/CIFAR10', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_size = len(train_dataset)\n",
    "total_test_size = len(test_dataset)\n",
    "classes = 10\n",
    "input_dim = 784\n",
    "num_clients = 10\n",
    "rounds = 30\n",
    "batch_size = 32\n",
    "in_group_rounds=3\n",
    "epochs_per_client =10\n",
    "learning_rate = 8e-2\n",
    "num_malicious_clients=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "def average_models(new_model, next, scale):\n",
    "    if new_model == None:\n",
    "        new_model = next\n",
    "        for key in new_model:\n",
    "            new_model[key] = new_model[key] * scale\n",
    "    else:\n",
    "        for key in new_model:\n",
    "            new_model[key] = new_model[key] + (next[key] * scale)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedNet(torch.nn.Module):    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Update input channels to 3 for RGB images\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 16, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        \n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        \n",
    "        self.fc = torch.nn.Linear(8 * 8 * 32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    def batch_accuracy(self, outputs, labels):\n",
    "        with torch.no_grad():\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "    \n",
    "    def _process_batch(self, batch):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        accuracy = self.batch_accuracy(outputs, labels)\n",
    "        return (loss, accuracy)\n",
    "    \n",
    "    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n",
    "        optimizer = opt(self.parameters(), lr)\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            accs = []\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss.detach()\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "            avg_loss = torch.stack(losses).mean().item()\n",
    "            avg_acc = torch.stack(accs).mean().item()\n",
    "            history.append((avg_loss, avg_acc))\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, dataset, batch_size=128):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n",
    "        losses = []\n",
    "        accs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "        avg_loss = torch.stack(losses).mean().item()\n",
    "        avg_acc = torch.stack(accs).mean().item()\n",
    "        return (avg_loss, avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data_label_quantity(\n",
    "    num_clients, labels_per_client, train_dataset,total_train_size,seed=42):\n",
    "    trainset=train_dataset\n",
    "    prng = np.random.default_rng(seed)\n",
    "\n",
    "    targets = trainset.targets\n",
    "    if isinstance(targets, list):\n",
    "        targets = np.array(targets)\n",
    "    if isinstance(targets, torch.Tensor):\n",
    "        targets = targets.numpy()\n",
    "    num_classes = len(set(targets))\n",
    "    times = [0 for _ in range(num_classes)]\n",
    "    contains = []\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        current = [i % num_classes]\n",
    "        times[i % num_classes] += 1\n",
    "        j = 1\n",
    "        while j < labels_per_client:\n",
    "            index = prng.choice(num_classes, 1)[0]\n",
    "            if index not in current:\n",
    "                current.append(index)\n",
    "                times[index] += 1\n",
    "                j += 1\n",
    "        contains.append(current)\n",
    "    idx_clients: List[List] = [[] for _ in range(num_clients)]\n",
    "    for i in range(num_classes):\n",
    "        idx_k = np.where(targets == i)[0]\n",
    "        prng.shuffle(idx_k)\n",
    "        idx_k_split = np.array_split(idx_k, times[i])\n",
    "        ids = 0\n",
    "        for j in range(num_clients):\n",
    "            if i in contains[j]:\n",
    "                idx_clients[j] += idx_k_split[ids].tolist()\n",
    "                ids += 1\n",
    "    trainsets_per_client = [Subset(trainset, idxs) for idxs in idx_clients]\n",
    "    return trainsets_per_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_datasets = partition_data_label_quantity(num_clients,2,train_dataset,len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class memory:\n",
    "\n",
    "    def __init__(self, batch_number, batch_size):\n",
    "        self.batch_number = batch_number\n",
    "        self.batch_size = batch_size\n",
    "        self.matrix = np.zeros((batch_number, batch_size), dtype=int)\n",
    "        self.add_time = 0\n",
    "\n",
    "    def add_number(self, batch_no, true_array):\n",
    "        if batch_no >= self.batch_number:\n",
    "            print(f\"Warning: Batch number {batch_no} out of range. Skipping this entry.\")\n",
    "            return\n",
    "        # Ensure true_array length matches batch_size by padding or truncating as necessary\n",
    "        true_array = np.pad(true_array, (0, max(0, self.batch_size - len(true_array))), mode='constant')[:self.batch_size]\n",
    "        self.matrix[batch_no] += true_array\n",
    "\n",
    "\n",
    "    def add(self):\n",
    "        self.add_time = self.add_time + 1\n",
    "\n",
    "    def get_ambiguous(self, number):\n",
    "        temp_matrix = self.matrix\n",
    "        temp_matrix = np.abs(temp_matrix - np.ones(self.matrix.shape) * self.add_time * 0.5)\n",
    "        temp_array = np.reshape(temp_matrix, self.matrix.shape[0] * self.matrix.shape[1])\n",
    "        temp = list(map(list, zip(range(len(temp_array)), temp_array)))\n",
    "        small = sorted(temp, key=lambda x: x[1], reverse=False)\n",
    "        small_array = []\n",
    "        for i in range(number):\n",
    "            small_array.append(small[i][0])\n",
    "        return small_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "# import dnn_model  # Assumed custom DNN model import\n",
    "# from memory import memory  # Assumed memory module import\n",
    "\n",
    "\n",
    "class KMT:\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.5\n",
    "    batch_size = 32\n",
    "    batch_number = 1000\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    def __init__(self, dataset, index, test_loader):\n",
    "        # Load training set and initialize test loader\n",
    "        self.load_train_set(dataset, index)\n",
    "        self.test_loader = test_loader\n",
    "        self.model = FederatedNet()  # Use FederatedNet model here\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.distillation_criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=self.momentum)\n",
    "        self.memory = memory(self.batch_number, self.batch_size)\n",
    "\n",
    "    def load_train_set(self, dataset, index):\n",
    "        # Subset and DataLoader for training\n",
    "        train_dataset = torch.utils.data.Subset(dataset, index.astype(int))\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def train(self):\n",
    "        # Standard training without distillation\n",
    "        self.memory.add()  # Initialize memory\n",
    "        running_loss = 0\n",
    "        batch_number = 0\n",
    "\n",
    "        for images, labels in self.train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(images)\n",
    "            loss = self.criterion(output, labels)\n",
    "\n",
    "            # Accuracy calculation for memory tracking\n",
    "            log_ps = torch.log_softmax(output, dim=1)\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = (top_class == labels.view(*top_class.shape)).numpy().squeeze().astype(int).transpose()\n",
    "            self.memory.add_number(batch_number, equals)\n",
    "\n",
    "            # Backpropagation and optimizer step\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            batch_number += 1\n",
    "\n",
    "        self.train_loss.append(running_loss / len(self.train_loader))\n",
    "\n",
    "    def distillation_train(self, images, labels, other_soft_decision):\n",
    "        # Knowledge distillation training with soft labels from other clients\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(images)\n",
    "        loss1 = self.criterion(output, labels)\n",
    "        loss2 = self.distillation_criterion(torch.log_softmax(output, dim=1), other_soft_decision)\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        log_ps = torch.log_softmax(output, dim=1)\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = (top_class == labels.view(*top_class.shape)).numpy().squeeze().astype(int).transpose()\n",
    "\n",
    "        # Backpropagation and optimizer step\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return equals\n",
    "\n",
    "    def test(self):\n",
    "        accuracy = 0\n",
    "        loss = 0\n",
    "        for images, labels in self.test_loader:\n",
    "            output = self.model(images)\n",
    "            log_ps = torch.log_softmax(output, dim=1)\n",
    "            ps = torch.exp(log_ps)\n",
    "            loss += self.criterion(log_ps, labels)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        self.test_loss.append((loss / len(self.test_loader)).item())\n",
    "        self.test_accuracy.append((accuracy / len(self.test_loader)).item())\n",
    "        return accuracy / len(self.test_loader)\n",
    "\n",
    "    def save_model(self, filename=\"temp_model.pt\"):\n",
    "        torch.save(self.model.state_dict(), filename)\n",
    "\n",
    "    def load_model(self, filename=\"temp_model.pt\"):\n",
    "        self.model.load_state_dict(torch.load(filename))\n",
    "\n",
    "    def get_ambiguous_data(self, array):\n",
    "        np.sort(array)\n",
    "        batch_number = 0\n",
    "        ambiguous_images = []\n",
    "        ambiguous_labels = []\n",
    "        for images, labels in self.train_loader:\n",
    "            for no in array:\n",
    "                if int(no / self.batch_size) == batch_number:\n",
    "                    column = np.mod(no, self.batch_size)\n",
    "                    ambiguous_images.append(images[column])\n",
    "                    ambiguous_labels.append(labels[column])\n",
    "            batch_number += 1\n",
    "        ambiguous_images = torch.stack(ambiguous_images)\n",
    "        ambiguous_labels = torch.stack(ambiguous_labels)\n",
    "        return ambiguous_images, ambiguous_labels\n",
    "\n",
    "    def test_data(self, images, labels):\n",
    "        loss = 0\n",
    "        output = self.model(images)\n",
    "        log_ps = torch.log_softmax(output, dim=1)\n",
    "        loss += self.criterion(log_ps, labels)\n",
    "        return loss\n",
    "\n",
    "    def test_data_accuracy(self, images, labels):\n",
    "        output = self.model(images)\n",
    "        log_ps = torch.log_softmax(output, dim=1)\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, dataset, test_loader):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "        # self.test_loader = test_loader\n",
    "        self.kmt = KMT(dataset, index=np.arange(len(dataset)), test_loader=test_loader)\n",
    "        self.client_data = []\n",
    "\n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "\n",
    "    def train(self, parameters_dict, other_soft_decisions=None):\n",
    "        # Load initial parameters\n",
    "        self.kmt.model.load_state_dict(parameters_dict)\n",
    "\n",
    "        # Standard training\n",
    "        self.kmt.train()\n",
    "\n",
    "        # Knowledge distillation using soft labels from other clients (if available)\n",
    "        if other_soft_decisions is not None:\n",
    "            for images, labels in self.kmt.train_loader:\n",
    "                if images in other_soft_decisions:\n",
    "                    other_soft_decision = other_soft_decisions[images]\n",
    "                    self.kmt.distillation_train(images, labels, other_soft_decision)\n",
    "        \n",
    "        # Return the updated model parameters\n",
    "        return self.kmt.model.state_dict()\n",
    "\n",
    "    def recieve_parameters(self, client_id, client_parameter, fraction):\n",
    "        client_object = {\"client_id\": client_id, \"client_parameter\": client_parameter, \"fraction\": fraction}\n",
    "        self.client_data.append(client_object)\n",
    "\n",
    "    def aggregated_parameters(self):\n",
    "        local_aggregated_model = None\n",
    "        for i in range(len(self.client_data)):\n",
    "            local_aggregated_model = average_models(local_aggregated_model, self.client_data[i][\"client_parameter\"], self.client_data[i][\"fraction\"])\n",
    "        self.client_data.clear()\n",
    "        return local_aggregated_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=200, shuffle=True)\n",
    "clients = [Client(i, client_datasets[i], test_loader) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After round 1, train_loss = 2.5177, dev_loss = 2.5257, dev_acc = 0.1548\n",
      "After round 2, train_loss = 2.0817, dev_loss = 2.084, dev_acc = 0.2277\n",
      "After round 3, train_loss = 1.8699, dev_loss = 1.8758, dev_acc = 0.308\n",
      "After round 4, train_loss = 1.7814, dev_loss = 1.7902, dev_acc = 0.3624\n",
      "After round 5, train_loss = 1.7268, dev_loss = 1.7353, dev_acc = 0.3721\n",
      "After round 6, train_loss = 1.7241, dev_loss = 1.7377, dev_acc = 0.3589\n",
      "After round 7, train_loss = 1.6462, dev_loss = 1.659, dev_acc = 0.4027\n"
     ]
    }
   ],
   "source": [
    "dfl_start_time = time.time()\n",
    "\n",
    "global_net = to_device(FederatedNet(), device)\n",
    "curr_parameters = global_net.state_dict()\n",
    "history_dfl = []\n",
    "weights = []\n",
    "traffic = []\n",
    "load_model = None\n",
    "total_data_size = len(train_dataset)\n",
    "\n",
    "for i in range(rounds):\n",
    "    new_model = None\n",
    "    soft_decisions = {}  # Dictionary to store each client's soft decisions for distillation\n",
    "\n",
    "    # Step 1: Each client performs training and generates soft decisions\n",
    "    for client in clients:\n",
    "        fraction = client.get_dataset_size() / total_data_size\n",
    "\n",
    "        # Each client trains on its dataset, possibly using other clients' soft decisions for distillation\n",
    "        client_parameters = client.train(curr_parameters, other_soft_decisions=soft_decisions if i > 0 else None)\n",
    "        \n",
    "        # Gather soft decisions for this client's data to use in distillation\n",
    "        for images, labels in client.kmt.train_loader:\n",
    "            output = client.kmt.model(images)\n",
    "            soft_decisions[images] = torch.softmax(output, dim=1).detach()\n",
    "\n",
    "        # Share parameters with other clients for aggregation\n",
    "        for clt in clients:\n",
    "            clt.recieve_parameters(client.get_client_id(), client_parameters, fraction)\n",
    "\n",
    "        # Aggregate models across all clients\n",
    "        new_model = average_models(new_model, client_parameters, fraction)\n",
    "\n",
    "    # Update the global model and evaluate\n",
    "    load_model = new_model.copy()\n",
    "    global_net.load_state_dict(new_model)\n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    dev_loss, dev_acc = global_net.evaluate(test_dataset)\n",
    "    \n",
    "    # Logging\n",
    "    print(f'After round {i + 1}, train_loss = {round(train_loss, 4)}, dev_loss = {round(dev_loss, 4)}, dev_acc = {round(dev_acc, 4)}')\n",
    "    history_dfl.append((train_acc, dev_acc, i))\n",
    "    traffic.append((num_clients) * (num_clients - 1))\n",
    "\n",
    "# Calculate total time\n",
    "dfl_end_time = time.time()\n",
    "dfl_time = dfl_end_time - dfl_start_time\n",
    "print(f\"Total training time: {dfl_time:.2f} seconds\")\n",
    "\n",
    "# Optionally save the final global model if needed\n",
    "torch.save(global_net.state_dict(), \"final_global_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history_dfl))], [history_dfl[i][0] for i in range(len(history_dfl))], color='r', label='train accuracy')\n",
    "plt.plot([i + 1 for i in range(len(history_dfl))], [history_dfl[i][1] for i in range(len(history_dfl))], color='b', label='dev accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Rounds vs Accuracy\")\n",
    "plt.xlabel(\"Number of rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, dataset, test_loader):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "        self.kmt = KMT(dataset, index=np.arange(len(dataset)), test_loader=test_loader)\n",
    "        self.client_data=[]\n",
    "        self.server_id=-1\n",
    "        self.distance=random.randint(1,100)\n",
    "\n",
    "    def set_server_id(self,client_id):\n",
    "        self.client_data_server=[]\n",
    "        self.server_id=client_id\n",
    "    \n",
    "    def get_server_id(self):\n",
    "        return self.server_id\n",
    "\n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "\n",
    "    def train(self, parameters_dict, other_soft_decisions=None):\n",
    "        # Load initial parameters\n",
    "        self.kmt.model.load_state_dict(parameters_dict)\n",
    "\n",
    "        # Standard training\n",
    "        self.kmt.train()\n",
    "\n",
    "        # Knowledge distillation using soft labels from other clients (if available)\n",
    "        if other_soft_decisions is not None:\n",
    "            for images, labels in self.kmt.train_loader:\n",
    "                if images in other_soft_decisions:\n",
    "                    other_soft_decision = other_soft_decisions[images]\n",
    "                    self.kmt.distillation_train(images, labels, other_soft_decision)\n",
    "        \n",
    "        # Return the updated model parameters\n",
    "        return self.kmt.model.state_dict()\n",
    "\n",
    "    def recieve_parameters(self, client_id, client_parameter, fraction):\n",
    "        client_object = {\"client_id\": client_id, \"client_parameter\": client_parameter, \"fraction\": fraction}\n",
    "        self.client_data.append(client_object)\n",
    "\n",
    "    def aggregated_parameters(self):\n",
    "        local_aggregated_model = None\n",
    "        for i in range(len(self.client_data)):\n",
    "            local_aggregated_model = average_models(local_aggregated_model, self.client_data[i][\"client_parameter\"], self.client_data[i][\"fraction\"])\n",
    "        self.client_data.clear()\n",
    "        return local_aggregated_model\n",
    "    \n",
    "    def recieve_data_as_server(self,client_parameter,fraction):\n",
    "        client_object={\"client_parameter\":client_parameter,\"fraction\":fraction}\n",
    "        self.client_data_server.append(client_object)\n",
    "    \n",
    "\n",
    "    def aggregate_parameter_as_server(self):\n",
    "         local_aggregated_model_as_server=None\n",
    "         for data in self.client_data_server:\n",
    "              local_aggregated_model_as_server=average_models(local_aggregated_model_as_server,data[\"client_parameter\"],data[\"fraction\"])\n",
    "         self.client_data_server.clear()\n",
    "         return local_aggregated_model_as_server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_function(client_set_1,client_set_2,data):\n",
    "  client_set_1_average=None\n",
    "  client_set_2_average=None\n",
    "  for i in client_set_1:\n",
    "    client_set_1_average=average_models(client_set_1_average,data[i],1)\n",
    "  for i in client_set_2:\n",
    "    client_set_2_average=average_models(client_set_2_average,data[i],1)\n",
    "  model1_weights = []\n",
    "  model2_weights = []\n",
    "  for key, value in client_set_1_average.items():\n",
    "      model1_weights.append(value.detach().cpu())\n",
    "\n",
    "  for key, value in client_set_2_average.items():\n",
    "      model2_weights.append(value.detach().cpu())\n",
    "  weight1_flat = torch.cat([param.view(-1) for param in model1_weights])\n",
    "  weight2_flat = torch.cat([param.view(-1) for param in model2_weights])\n",
    "  cosine_sim = torch.nn.functional.cosine_similarity(weight1_flat, weight2_flat, dim=0)\n",
    "  return cosine_sim.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateGroupsGame(data,num_clients):\n",
    "    groups=[]\n",
    "    for i in range(0,num_clients):\n",
    "        groups.append([i])\n",
    "    while True:\n",
    "        for group in groups:\n",
    "            for x in group:\n",
    "                no_change_flag=0\n",
    "                m_i_n_group=None\n",
    "                m_i_n_utility=100\n",
    "                minu=0\n",
    "                for g in groups:\n",
    "                    if g!=group:\n",
    "                        if len(g) >= 1 and len(group) > 1:\n",
    "                            tempset_1=g.copy()\n",
    "                            tempset_1.append(x)\n",
    "                            tempset_2=group.copy()\n",
    "                            tempset_2.remove(x)\n",
    "                            if utility_function(tempset_1,tempset_2,data) > utility_function(g,group,data): \n",
    "                                if minu < utility_function(tempset_1,tempset_2,data):\n",
    "                                    m_i_n_group=g\n",
    "                                    minu=utility_function(tempset_1,tempset_2,data)        \n",
    "                        elif len(group)<=1:\n",
    "                            if utility_function(g,group,data) < m_i_n_utility:\n",
    "                                m_i_n_group=g\n",
    "                                m_i_n_utility=utility_function(g,group,data)\n",
    "                if m_i_n_group is not None:  \n",
    "                    group.remove(x)\n",
    "                    m_i_n_group.append(x)\n",
    "                    no_change_flag=1\n",
    "                    if len(group) <=0 :\n",
    "                        groups.remove(group)\n",
    "        if no_change_flag == 0:\n",
    "            break\n",
    "    print(\"end\")\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupDataLength(groups,clients):\n",
    "    group_dataset_length=[]\n",
    "    for grp in groups:\n",
    "        totalcount=0\n",
    "        for clt in grp:\n",
    "            totalcount+=clients[clt].get_dataset_size()\n",
    "        group_dataset_length.append(totalcount)\n",
    "    return group_dataset_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=200, shuffle=True)\n",
    "clients = [Client(i, client_datasets[i], test_loader) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_net = to_device(FederatedNet(), device)\n",
    "curr_parameters = global_net.state_dict()\n",
    "data = []\n",
    "for client in clients:\n",
    "    paramerter=client.train(curr_parameters)\n",
    "    data.append(paramerter)\n",
    "\n",
    "groups=generateGroupsGame(data,num_clients)\n",
    "print(groups)\n",
    "group_dataset_length=groupDataLength(groups,clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfl_start_time = time.time()\n",
    "\n",
    "global_net = to_device(FederatedNet(), device)\n",
    "curr_parameters = global_net.state_dict()\n",
    "history_hdfl = []\n",
    "weights = []\n",
    "traffic = []\n",
    "load_model = None\n",
    "total_data_size = len(train_dataset)\n",
    "\n",
    "for i in range(rounds):\n",
    "    new_model = None\n",
    "    soft_decisions = {}  # Dictionary to store each client's soft decisions for distillation\n",
    "\n",
    "    # Step 1: Each client performs training and generates soft decisions\n",
    "    for client in clients:\n",
    "        fraction = client.get_dataset_size() / total_data_size\n",
    "\n",
    "        # Each client trains on its dataset, possibly using other clients' soft decisions for distillation\n",
    "        client_parameters = client.train(curr_parameters, other_soft_decisions=soft_decisions if i > 0 else None)\n",
    "        \n",
    "        # Gather soft decisions for this client's data to use in distillation\n",
    "        for images, labels in client.kmt.train_loader:\n",
    "            output = client.kmt.model(images)\n",
    "            soft_decisions[images] = torch.softmax(output, dim=1).detach()\n",
    "\n",
    "        # Share parameters with other clients for aggregation\n",
    "        for clt in clients:\n",
    "            clt.recieve_parameters(client.get_client_id(), client_parameters, fraction)\n",
    "\n",
    "        # Aggregate models across all clients\n",
    "        new_model = average_models(new_model, client_parameters, fraction)\n",
    "\n",
    "    # Update the global model and evaluate\n",
    "    load_model = new_model.copy()\n",
    "    global_net.load_state_dict(new_model)\n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    dev_loss, dev_acc = global_net.evaluate(test_dataset)\n",
    "    \n",
    "    # Logging\n",
    "    print(f'After round {i + 1}, train_loss = {round(train_loss, 4)}, dev_loss = {round(dev_loss, 4)}, dev_acc = {round(dev_acc, 4)}')\n",
    "    history_hdfl.append((train_acc, dev_acc, i))\n",
    "    traffic.append((num_clients) * (num_clients - 1))\n",
    "\n",
    "# Calculate total time\n",
    "hdfl_end_time = time.time()\n",
    "hdfl_time = hdfl_end_time - hdfl_start_time\n",
    "print(f\"Total training time: {hdfl_time:.2f} seconds\")\n",
    "\n",
    "# Optionally save the final global model if needed\n",
    "torch.save(global_net.state_dict(), \"final_global_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history_hdfl))], [history_hdfl[i][0] for i in range(len(history_hdfl))], color='r', label='train accuracy')\n",
    "plt.plot([i + 1 for i in range(len(history_hdfl))], [history_hdfl[i][1] for i in range(len(history_hdfl))], color='b', label='dev accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Rounds vs Accuracy\")\n",
    "plt.xlabel(\"Number of rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history_dfl))], [history_dfl[i][1] for i in range(len(history_dfl))], color='b', label='DFL dev accuracy')\n",
    "plt.plot([i + 1 for i in range(len(history_hdfl))], [history_hdfl[i][1] for i in range(len(history_hdfl))], color='r', label='HDFL dev accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Rounds vs Accuracy on CIFAR10\")\n",
    "plt.xlabel(\"Number of rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['DFL', 'HDFL']\n",
    "values = [dfl_time, hdfl_time]\n",
    "\n",
    "# Plot the bar graph\n",
    "plt.bar(labels, values, color=['green', 'red'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('')\n",
    "plt.ylabel('RUN TIME')\n",
    "plt.title('Run Time for DFL & HDFL')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
