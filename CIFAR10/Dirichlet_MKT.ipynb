{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import pickle\n",
    "import time\n",
    "from torch.utils.data import ConcatDataset, Dataset, Subset\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CIFAR10('D:/downloads/btp/outputforpaper/CIFAR10', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = CIFAR10('D:/downloads/btp/outputforpaper/CIFAR10', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_size = len(train_dataset)\n",
    "total_test_size = len(test_dataset)\n",
    "classes = 10\n",
    "input_dim = 784\n",
    "num_clients = 10\n",
    "rounds = 30\n",
    "batch_size = 32\n",
    "in_group_rounds=3\n",
    "epochs_per_client =10\n",
    "learning_rate = 2e-2\n",
    "num_malicious_clients=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "def average_models(new_model, next, scale):\n",
    "    if new_model == None:\n",
    "        new_model = next\n",
    "        for key in new_model:\n",
    "            new_model[key] = new_model[key] * scale\n",
    "    else:\n",
    "        for key in new_model:\n",
    "            new_model[key] = new_model[key] + (next[key] * scale)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedNet(torch.nn.Module):    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Update input channels to 3 for RGB images\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 16, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        \n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm2d(32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2))\n",
    "        \n",
    "        self.fc = torch.nn.Linear(8 * 8 * 32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    def batch_accuracy(self, outputs, labels):\n",
    "        with torch.no_grad():\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "    \n",
    "    def _process_batch(self, batch):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        accuracy = self.batch_accuracy(outputs, labels)\n",
    "        return (loss, accuracy)\n",
    "    \n",
    "    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n",
    "        optimizer = opt(self.parameters(), lr)\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            accs = []\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss.detach()\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "            avg_loss = torch.stack(losses).mean().item()\n",
    "            avg_acc = torch.stack(accs).mean().item()\n",
    "            history.append((avg_loss, avg_acc))\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, dataset, batch_size=128):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n",
    "        losses = []\n",
    "        accs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "        avg_loss = torch.stack(losses).mean().item()\n",
    "        avg_acc = torch.stack(accs).mean().item()\n",
    "        return (avg_loss, avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data_dirichlet(num_clients,train_dataset,total_train_size,alpha, seed=42): \n",
    "    trainset=train_dataset\n",
    "    min_required_samples_per_client = 10\n",
    "    min_samples = 0\n",
    "    prng = np.random.default_rng(seed)\n",
    "\n",
    "    # get the targets\n",
    "    tmp_t = trainset.targets\n",
    "    if isinstance(tmp_t, list):\n",
    "        tmp_t = np.array(tmp_t)\n",
    "    if isinstance(tmp_t, torch.Tensor):\n",
    "        tmp_t = tmp_t.numpy()\n",
    "    num_classes = len(set(tmp_t))\n",
    "    total_samples = len(tmp_t)\n",
    "    while min_samples < min_required_samples_per_client:\n",
    "        idx_clients: List[List] = [[] for _ in range(num_clients)]\n",
    "        for k in range(num_classes):\n",
    "            idx_k = np.where(tmp_t == k)[0]\n",
    "            prng.shuffle(idx_k)\n",
    "            proportions = prng.dirichlet(np.repeat(alpha, num_clients))\n",
    "            proportions = np.array(\n",
    "                [\n",
    "                    p * (len(idx_j) < total_samples / num_clients)\n",
    "                    for p, idx_j in zip(proportions, idx_clients)\n",
    "                ]\n",
    "            )\n",
    "            proportions = proportions / proportions.sum()\n",
    "            proportions = (np.cumsum(proportions) * len(idx_k)).astype(int)[:-1]\n",
    "            idx_k_split = np.split(idx_k, proportions)\n",
    "            idx_clients = [\n",
    "                idx_j + idx.tolist() for idx_j, idx in zip(idx_clients, idx_k_split)\n",
    "            ]\n",
    "            min_samples = min([len(idx_j) for idx_j in idx_clients])\n",
    "\n",
    "    trainsets_per_client = [Subset(trainset, idxs) for idxs in idx_clients]\n",
    "    return trainsets_per_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_datasets = partition_data_dirichlet(num_clients,train_dataset,len(train_dataset),0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class memory:\n",
    "\n",
    "    def __init__(self, batch_number, batch_size):\n",
    "        self.batch_number = batch_number\n",
    "        self.batch_size = batch_size\n",
    "        self.matrix = np.zeros((batch_number, batch_size), dtype=int)\n",
    "        self.add_time = 0\n",
    "\n",
    "    def add_number(self, batch_no, true_array):\n",
    "        if batch_no >= self.batch_number:\n",
    "            print(f\"Warning: Batch number {batch_no} out of range. Skipping this entry.\")\n",
    "            return\n",
    "        # Ensure true_array length matches batch_size by padding or truncating as necessary\n",
    "        true_array = np.pad(true_array, (0, max(0, self.batch_size - len(true_array))), mode='constant')[:self.batch_size]\n",
    "        self.matrix[batch_no] += true_array\n",
    "\n",
    "\n",
    "    def add(self):\n",
    "        self.add_time = self.add_time + 1\n",
    "\n",
    "    def get_ambiguous(self, number):\n",
    "        temp_matrix = self.matrix\n",
    "        temp_matrix = np.abs(temp_matrix - np.ones(self.matrix.shape) * self.add_time * 0.5)\n",
    "        temp_array = np.reshape(temp_matrix, self.matrix.shape[0] * self.matrix.shape[1])\n",
    "        temp = list(map(list, zip(range(len(temp_array)), temp_array)))\n",
    "        small = sorted(temp, key=lambda x: x[1], reverse=False)\n",
    "        small_array = []\n",
    "        for i in range(number):\n",
    "            small_array.append(small[i][0])\n",
    "        return small_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "# import dnn_model  # Assumed custom DNN model import\n",
    "# from memory import memory  # Assumed memory module import\n",
    "\n",
    "\n",
    "class KMT:\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.5\n",
    "    batch_size = 32\n",
    "    batch_number = 1000\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    def __init__(self, dataset, index, test_loader):\n",
    "        # Load training set and initialize test loader\n",
    "        self.load_train_set(dataset, index)\n",
    "        self.test_loader = test_loader\n",
    "        self.model = FederatedNet()  # Use FederatedNet model here\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.distillation_criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=self.momentum)\n",
    "        self.memory = memory(self.batch_number, self.batch_size)\n",
    "\n",
    "    def load_train_set(self, dataset, index):\n",
    "        # Subset and DataLoader for training\n",
    "        train_dataset = torch.utils.data.Subset(dataset, index.astype(int))\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def train(self):\n",
    "        # Standard training without distillation\n",
    "        self.memory.add()  # Initialize memory\n",
    "        running_loss = 0\n",
    "        batch_number = 0\n",
    "\n",
    "        for images, labels in self.train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(images)\n",
    "            loss = self.criterion(output, labels)\n",
    "\n",
    "            # Accuracy calculation for memory tracking\n",
    "            log_ps = torch.log_softmax(output, dim=1)\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = (top_class == labels.view(*top_class.shape)).numpy().squeeze().astype(int).transpose()\n",
    "            self.memory.add_number(batch_number, equals)\n",
    "\n",
    "            # Backpropagation and optimizer step\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            batch_number += 1\n",
    "\n",
    "        self.train_loss.append(running_loss / len(self.train_loader))\n",
    "\n",
    "    def distillation_train(self, images, labels, other_soft_decision):\n",
    "        # Knowledge distillation training with soft labels from other clients\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(images)\n",
    "        loss1 = self.criterion(output, labels)\n",
    "        loss2 = self.distillation_criterion(torch.log_softmax(output, dim=1), other_soft_decision)\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        log_ps = torch.log_softmax(output, dim=1)\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = (top_class == labels.view(*top_class.shape)).numpy().squeeze().astype(int).transpose()\n",
    "\n",
    "        # Backpropagation and optimizer step\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return equals\n",
    "\n",
    "    def test(self):\n",
    "        accuracy = 0\n",
    "        loss = 0\n",
    "        for images, labels in self.test_loader:\n",
    "            output = self.model(images)\n",
    "            log_ps = torch.log_softmax(output, dim=1)\n",
    "            ps = torch.exp(log_ps)\n",
    "            loss += self.criterion(log_ps, labels)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        self.test_loss.append((loss / len(self.test_loader)).item())\n",
    "        self.test_accuracy.append((accuracy / len(self.test_loader)).item())\n",
    "        return accuracy / len(self.test_loader)\n",
    "\n",
    "    def save_model(self, filename=\"temp_model.pt\"):\n",
    "        torch.save(self.model.state_dict(), filename)\n",
    "\n",
    "    def load_model(self, filename=\"temp_model.pt\"):\n",
    "        self.model.load_state_dict(torch.load(filename))\n",
    "\n",
    "    def get_ambiguous_data(self, array):\n",
    "        np.sort(array)\n",
    "        batch_number = 0\n",
    "        ambiguous_images = []\n",
    "        ambiguous_labels = []\n",
    "        for images, labels in self.train_loader:\n",
    "            for no in array:\n",
    "                if int(no / self.batch_size) == batch_number:\n",
    "                    column = np.mod(no, self.batch_size)\n",
    "                    ambiguous_images.append(images[column])\n",
    "                    ambiguous_labels.append(labels[column])\n",
    "            batch_number += 1\n",
    "        ambiguous_images = torch.stack(ambiguous_images)\n",
    "        ambiguous_labels = torch.stack(ambiguous_labels)\n",
    "        return ambiguous_images, ambiguous_labels\n",
    "\n",
    "    def test_data(self, images, labels):\n",
    "        loss = 0\n",
    "        output = self.model(images)\n",
    "        log_ps = torch.log_softmax(output, dim=1)\n",
    "        loss += self.criterion(log_ps, labels)\n",
    "        return loss\n",
    "\n",
    "    def test_data_accuracy(self, images, labels):\n",
    "        output = self.model(images)\n",
    "        log_ps = torch.log_softmax(output, dim=1)\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, dataset, test_loader):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "        # self.test_loader = test_loader\n",
    "        self.kmt = KMT(dataset, index=np.arange(len(dataset)), test_loader=test_loader)\n",
    "        self.client_data = []\n",
    "\n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "\n",
    "    def train(self, parameters_dict, other_soft_decisions=None):\n",
    "        # Load initial parameters\n",
    "        self.kmt.model.load_state_dict(parameters_dict)\n",
    "\n",
    "        # Standard training\n",
    "        self.kmt.train()\n",
    "\n",
    "        # Knowledge distillation using soft labels from other clients (if available)\n",
    "        if other_soft_decisions is not None:\n",
    "            for images, labels in self.kmt.train_loader:\n",
    "                if images in other_soft_decisions:\n",
    "                    other_soft_decision = other_soft_decisions[images]\n",
    "                    self.kmt.distillation_train(images, labels, other_soft_decision)\n",
    "        \n",
    "        # Return the updated model parameters\n",
    "        return self.kmt.model.state_dict()\n",
    "\n",
    "    def recieve_parameters(self, client_id, client_parameter, fraction):\n",
    "        client_object = {\"client_id\": client_id, \"client_parameter\": client_parameter, \"fraction\": fraction}\n",
    "        self.client_data.append(client_object)\n",
    "\n",
    "    def aggregated_parameters(self):\n",
    "        local_aggregated_model = None\n",
    "        for i in range(len(self.client_data)):\n",
    "            local_aggregated_model = average_models(local_aggregated_model, self.client_data[i][\"client_parameter\"], self.client_data[i][\"fraction\"])\n",
    "        self.client_data.clear()\n",
    "        return local_aggregated_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=200, shuffle=True)\n",
    "clients = [Client(i, client_datasets[i], test_loader) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m fraction \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_dataset_size() \u001b[38;5;241m/\u001b[39m total_data_size\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Each client trains on its dataset, possibly using other clients' soft decisions for distillation\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m     client_parameters \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_soft_decisions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msoft_decisions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# Each client trains on its dataset, possibly using other clients' soft decisions for distillation\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     client_parameters \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mtrain(client\u001b[38;5;241m.\u001b[39maggregated_parameters(), other_soft_decisions\u001b[38;5;241m=\u001b[39msoft_decisions)\n",
      "Cell \u001b[0;32mIn[17], line 20\u001b[0m, in \u001b[0;36mClient.train\u001b[0;34m(self, parameters_dict, other_soft_decisions)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkmt\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(parameters_dict)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Standard training\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkmt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Knowledge distillation using soft labels from other clients (if available)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m other_soft_decisions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[16], line 49\u001b[0m, in \u001b[0;36mKMT.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m top_p, top_class \u001b[38;5;241m=\u001b[39m ps\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;241m1\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m equals \u001b[38;5;241m=\u001b[39m (top_class \u001b[38;5;241m==\u001b[39m labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mtop_class\u001b[38;5;241m.\u001b[39mshape))\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_number\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_number\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mequals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimizer step\u001b[39;00m\n\u001b[1;32m     52\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m, in \u001b[0;36mmemory.add_number\u001b[0;34m(self, batch_no, true_array)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Ensure true_array length matches batch_size by padding or truncating as necessary\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m true_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mpad(true_array, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m-\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrue_array\u001b[49m\u001b[43m)\u001b[49m)), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m'\u001b[39m)[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmatrix[batch_no] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m true_array\n",
      "\u001b[0;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "dfl_start_time = time.time()\n",
    "\n",
    "global_net = to_device(FederatedNet(), device)\n",
    "curr_parameters = global_net.state_dict()\n",
    "history_dfl = []\n",
    "weights = []\n",
    "traffic = []\n",
    "load_model = None\n",
    "total_data_size = len(train_dataset)\n",
    "\n",
    "for i in range(rounds):\n",
    "    new_model = None\n",
    "    soft_decisions = {}  # Dictionary to store each client's soft decisions for distillation\n",
    "\n",
    "    # Step 1: Each client performs training and generates soft decisions\n",
    "    for client in clients:\n",
    "        fraction = client.get_dataset_size() / total_data_size\n",
    "\n",
    "        if i == 0:\n",
    "            # Each client trains on its dataset, possibly using other clients' soft decisions for distillation\n",
    "            client_parameters = client.train(curr_parameters, other_soft_decisions=soft_decisions if i > 0 else None)\n",
    "        else:\n",
    "            # Each client trains on its dataset, possibly using other clients' soft decisions for distillation\n",
    "            client_parameters = client.train(client.aggregated_parameters(), other_soft_decisions=soft_decisions)\n",
    "        \n",
    "        # Gather soft decisions for this client's data to use in distillation\n",
    "        for images, labels in client.kmt.train_loader:\n",
    "            output = client.kmt.model(images)\n",
    "            soft_decisions[images] = torch.softmax(output, dim=1).detach()\n",
    "\n",
    "        # Share parameters with other clients for aggregation\n",
    "        for clt in clients:\n",
    "            clt.recieve_parameters(client.get_client_id(), client_parameters, fraction)\n",
    "\n",
    "        # Aggregate models across all clients\n",
    "        new_model = average_models(new_model, client_parameters, fraction)\n",
    "\n",
    "    # Update the global model and evaluate\n",
    "    load_model = new_model.copy()\n",
    "    global_net.load_state_dict(new_model)\n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    dev_loss, dev_acc = global_net.evaluate(test_dataset)\n",
    "    \n",
    "    # Logging\n",
    "    print(f'After round {i + 1}, train_loss = {round(train_loss, 4)}, dev_loss = {round(dev_loss, 4)}, dev_acc = {round(dev_acc, 4)}')\n",
    "    history_dfl.append((train_acc, dev_acc, i))\n",
    "    traffic.append((num_clients) * (num_clients - 1))\n",
    "\n",
    "# Calculate total time\n",
    "dfl_end_time = time.time()\n",
    "dfl_time = dfl_end_time - dfl_start_time\n",
    "print(f\"Total training time: {dfl_time:.2f} seconds\")\n",
    "\n",
    "# Optionally save the final global model if needed\n",
    "torch.save(global_net.state_dict(), \"final_global_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history_dfl))], [history_dfl[i][0] for i in range(len(history_dfl))], color='r', label='train accuracy')\n",
    "plt.plot([i + 1 for i in range(len(history_dfl))], [history_dfl[i][1] for i in range(len(history_dfl))], color='b', label='dev accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Rounds vs Accuracy\")\n",
    "plt.xlabel(\"Number of rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, dataset, test_loader):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "        self.kmt = KMT(dataset, index=np.arange(len(dataset)), test_loader=test_loader)\n",
    "        self.client_data=[]\n",
    "        self.server_id=-1\n",
    "        self.distance=random.randint(1,100)\n",
    "\n",
    "    def set_server_id(self,client_id):\n",
    "        self.client_data_server=[]\n",
    "        self.server_id=client_id\n",
    "    \n",
    "    def get_server_id(self):\n",
    "        return self.server_id\n",
    "\n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "\n",
    "    def train(self, parameters_dict, other_soft_decisions=None):\n",
    "        # Load initial parameters\n",
    "        self.kmt.model.load_state_dict(parameters_dict)\n",
    "\n",
    "        # Standard training\n",
    "        self.kmt.train()\n",
    "\n",
    "        # Knowledge distillation using soft labels from other clients (if available)\n",
    "        if other_soft_decisions is not None:\n",
    "            for images, labels in self.kmt.train_loader:\n",
    "                if images in other_soft_decisions:\n",
    "                    other_soft_decision = other_soft_decisions[images]\n",
    "                    self.kmt.distillation_train(images, labels, other_soft_decision)\n",
    "        \n",
    "        # Return the updated model parameters\n",
    "        return self.kmt.model.state_dict()\n",
    "\n",
    "    def recieve_parameters(self, client_id, client_parameter, fraction):\n",
    "        client_object = {\"client_id\": client_id, \"client_parameter\": client_parameter, \"fraction\": fraction}\n",
    "        self.client_data.append(client_object)\n",
    "\n",
    "    def aggregated_parameters(self):\n",
    "        local_aggregated_model = None\n",
    "        for i in range(len(self.client_data)):\n",
    "            local_aggregated_model = average_models(local_aggregated_model, self.client_data[i][\"client_parameter\"], self.client_data[i][\"fraction\"])\n",
    "        self.client_data.clear()\n",
    "        return local_aggregated_model\n",
    "    \n",
    "    def recieve_data_as_server(self,client_parameter,fraction):\n",
    "        client_object={\"client_parameter\":client_parameter,\"fraction\":fraction}\n",
    "        self.client_data_server.append(client_object)\n",
    "    \n",
    "\n",
    "    def aggregate_parameter_as_server(self):\n",
    "         local_aggregated_model_as_server=None\n",
    "         for data in self.client_data_server:\n",
    "              local_aggregated_model_as_server=average_models(local_aggregated_model_as_server,data[\"client_parameter\"],data[\"fraction\"])\n",
    "         self.client_data_server.clear()\n",
    "         return local_aggregated_model_as_server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_function(client_set_1,client_set_2,data):\n",
    "  client_set_1_average=None\n",
    "  client_set_2_average=None\n",
    "  for i in client_set_1:\n",
    "    client_set_1_average=average_models(client_set_1_average,data[i],1)\n",
    "  for i in client_set_2:\n",
    "    client_set_2_average=average_models(client_set_2_average,data[i],1)\n",
    "  model1_weights = []\n",
    "  model2_weights = []\n",
    "  for key, value in client_set_1_average.items():\n",
    "      model1_weights.append(value.detach().cpu())\n",
    "\n",
    "  for key, value in client_set_2_average.items():\n",
    "      model2_weights.append(value.detach().cpu())\n",
    "  weight1_flat = torch.cat([param.view(-1) for param in model1_weights])\n",
    "  weight2_flat = torch.cat([param.view(-1) for param in model2_weights])\n",
    "  cosine_sim = torch.nn.functional.cosine_similarity(weight1_flat, weight2_flat, dim=0)\n",
    "  return cosine_sim.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateGroupsGame(data,num_clients):\n",
    "    groups=[]\n",
    "    for i in range(0,num_clients):\n",
    "        groups.append([i])\n",
    "    while True:\n",
    "        for group in groups:\n",
    "            for x in group:\n",
    "                no_change_flag=0\n",
    "                m_i_n_group=None\n",
    "                m_i_n_utility=100\n",
    "                minu=0\n",
    "                for g in groups:\n",
    "                    if g!=group:\n",
    "                        if len(g) >= 1 and len(group) > 1:\n",
    "                            tempset_1=g.copy()\n",
    "                            tempset_1.append(x)\n",
    "                            tempset_2=group.copy()\n",
    "                            tempset_2.remove(x)\n",
    "                            if utility_function(tempset_1,tempset_2,data) > utility_function(g,group,data): \n",
    "                                if minu < utility_function(tempset_1,tempset_2,data):\n",
    "                                    m_i_n_group=g\n",
    "                                    minu=utility_function(tempset_1,tempset_2,data)        \n",
    "                        elif len(group)<=1:\n",
    "                            if utility_function(g,group,data) < m_i_n_utility:\n",
    "                                m_i_n_group=g\n",
    "                                m_i_n_utility=utility_function(g,group,data)\n",
    "                if m_i_n_group is not None:  \n",
    "                    group.remove(x)\n",
    "                    m_i_n_group.append(x)\n",
    "                    no_change_flag=1\n",
    "                    if len(group) <=0 :\n",
    "                        groups.remove(group)\n",
    "        if no_change_flag == 0:\n",
    "            break\n",
    "    print(\"end\")\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupDataLength(groups,clients):\n",
    "    group_dataset_length=[]\n",
    "    for grp in groups:\n",
    "        totalcount=0\n",
    "        for clt in grp:\n",
    "            totalcount+=clients[clt].get_dataset_size()\n",
    "        group_dataset_length.append(totalcount)\n",
    "    return group_dataset_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=200, shuffle=True)\n",
    "clients = [Client(i, client_datasets[i], test_loader) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_net = to_device(FederatedNet(), device)\n",
    "curr_parameters = global_net.state_dict()\n",
    "data = []\n",
    "for client in clients:\n",
    "    paramerter=client.train(curr_parameters)\n",
    "    data.append(paramerter)\n",
    "\n",
    "groups=generateGroupsGame(data,num_clients)\n",
    "print(groups)\n",
    "group_dataset_length=groupDataLength(groups,clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfl_start_time  =time.time()\n",
    "\n",
    "# Initializing the global model and key variables\n",
    "global_net = to_device(FederatedNet(), device)\n",
    "curr_parameters = global_net.state_dict()\n",
    "history_hdfl = []\n",
    "weights = []\n",
    "traffic = []\n",
    "load_model = None\n",
    "total_dataset_length = len(train_dataset)\n",
    "flag = True\n",
    "\n",
    "# Training across rounds\n",
    "for i in range(rounds):\n",
    "\n",
    "    traffic_single_sum = 0\n",
    "    server = []\n",
    "    \n",
    "    # Step 1: Choose a random server from each group for group-level aggregation\n",
    "    for grp in groups:\n",
    "        traffic_single_sum += len(grp) - 1\n",
    "        server_id = np.random.choice(grp, p=np.ones(len(grp)) / len(grp))\n",
    "        server.append(server_id)\n",
    "        clients[server_id].set_server_id(server_id)\n",
    "    \n",
    "    k = 0  # Group index tracker\n",
    "    \n",
    "    # In-group mesh decentralized learning\n",
    "    for grp in groups:\n",
    "        flag = True\n",
    "        for j in range(in_group_rounds):\n",
    "            traffic_single_sum += (len(grp) - 1) * len(grp)\n",
    "\n",
    "            soft_decisions = {}  # Dictionary to store each client's soft decisions for distillation\n",
    "            \n",
    "            for clt in grp:\n",
    "                fraction = clients[clt].get_dataset_size() / group_dataset_length[k]\n",
    "                \n",
    "                # Train using the previous aggregated parameters or the current round model\n",
    "                if i == 0 and flag:\n",
    "                    client_parameters = clients[clt].train(curr_parameters, other_soft_decisions=soft_decisions if j > 0 else None)\n",
    "                elif j == 0:\n",
    "                    client_parameters = clients[clt].train(load_model, other_soft_decisions=soft_decisions if j > 0 else None)\n",
    "                else:\n",
    "                    client_parameters = clients[clt].train(clients[clt].aggregated_parameters(), other_soft_decisions=soft_decisions if j > 0 else None)\n",
    "                \n",
    "                # Generate and store soft decisions for knowledge distillation\n",
    "                soft_decisions = {}\n",
    "                for images, labels in clients[clt].kmt.train_loader:\n",
    "                    output = clients[clt].kmt.model(images)\n",
    "                    soft_decisions[images] = torch.softmax(output, dim=1).detach()\n",
    "                \n",
    "                # Share soft decisions and parameters with group members\n",
    "                for clt_1 in grp:\n",
    "                    clients[clt_1].recieve_parameters(clt_1, client_parameters, fraction)\n",
    "                    # clients[clt_1].kmt.update_soft_decisions(soft_decisions)\n",
    "            \n",
    "            flag = False\n",
    "        \n",
    "        # Aggregating on the chosen group server\n",
    "        for clt in grp:\n",
    "            fraction = clients[clt].get_dataset_size() / group_dataset_length[k]\n",
    "            group_params = clients[clt].aggregated_parameters()\n",
    "            clients[server[k]].recieve_data_as_server(group_params, fraction)\n",
    "        \n",
    "        k += 1\n",
    "    \n",
    "    # Step 2: Inter-group aggregation across group servers\n",
    "    aggregateds = []\n",
    "    for clt in server:\n",
    "        aggregated_params = clients[clt].aggregate_parameter_as_server()\n",
    "        aggregateds.append(aggregated_params)\n",
    "    \n",
    "    # Choose a central server among group servers for top-level aggregation\n",
    "    server_id = np.random.choice(server, p=np.ones(len(server)) / len(server))\n",
    "    clients[server_id].set_server_id(server_id)\n",
    "    traffic_single_sum += len(server)\n",
    "    \n",
    "    # Aggregate parameters at the top-level server and share with the main server\n",
    "    group_number = 0\n",
    "    for clt in aggregateds:\n",
    "        fraction = group_dataset_length[group_number] / total_dataset_length\n",
    "        group_number += 1\n",
    "        clients[server_id].recieve_data_as_server(clt, fraction)\n",
    "    \n",
    "    # Top-level aggregation on the main server\n",
    "    traffic_single_sum += num_clients\n",
    "    aggregated_parameters_top = clients[server_id].aggregate_parameter_as_server()\n",
    "    load_model = aggregated_parameters_top.copy()\n",
    "    \n",
    "    # Log weights and update the global model\n",
    "    total_norm = sum(torch.norm(value.view(-1)).item() for value in load_model.values())\n",
    "    weights.append(total_norm)\n",
    "    global_net.load_state_dict(load_model)\n",
    "    \n",
    "    # Evaluation after each round\n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    dev_loss, dev_acc = global_net.evaluate(test_dataset)\n",
    "    print(f'After round {i + 1}, train_loss = {round(train_loss, 4)}, dev_loss = {round(dev_loss, 4)}, dev_acc = {round(dev_acc, 4)}')\n",
    "    \n",
    "    traffic.append(traffic_single_sum)\n",
    "    history_hdfl.append((train_acc, dev_acc, i))\n",
    "\n",
    "# Calculate total time\n",
    "hdfl_end_time = time.time()\n",
    "hdfl_time = hdfl_end_time - hdfl_start_time\n",
    "print(f\"Total training time: {hdfl_time:.2f} seconds\")\n",
    "\n",
    "# Save the final model if needed\n",
    "torch.save(global_net.state_dict(), \"final_global_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history_hdfl))], [history_hdfl[i][0] for i in range(len(history_hdfl))], color='r', label='train accuracy')\n",
    "plt.plot([i + 1 for i in range(len(history_hdfl))], [history_hdfl[i][1] for i in range(len(history_hdfl))], color='b', label='dev accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Rounds vs Accuracy\")\n",
    "plt.xlabel(\"Number of rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history_dfl))], [history_dfl[i][1] for i in range(len(history_dfl))], color='b', label='dev accuracy')\n",
    "plt.plot([i + 1 for i in range(len(history_hdfl))], [history_hdfl[i][1] for i in range(len(history_hdfl))], color='r', label='dev accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Rounds vs Accuracy on CIFAR10\")\n",
    "plt.xlabel(\"Number of rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['DFL', 'HDFL']\n",
    "values = [dfl_time, hdfl_time]\n",
    "\n",
    "# Plot the bar graph\n",
    "plt.bar(labels, values, color=['green', 'red'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('')\n",
    "plt.ylabel('RUN TIME')\n",
    "plt.title('Run Time for DFL & HDFL')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
