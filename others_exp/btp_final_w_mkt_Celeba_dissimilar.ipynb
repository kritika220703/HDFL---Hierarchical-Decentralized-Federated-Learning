{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import CelebA\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import pickle\n",
    "import time\n",
    "from torch.utils.data import ConcatDataset, Dataset, Subset\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2422.0 bytes\n",
      "/home/vishal1/miniconda3/envs/nitinmtp/lib/python3.9/site-packages/torchvision/datasets/utils.py:260: UserWarning: We detected some HTML elements in the downloaded file. This most likely means that the download triggered an unhandled API response by GDrive. Please report this to torchvision at https://github.com/pytorch/vision/issues including the response:\n",
      "\n",
      "<!DOCTYPE html><html><head><title>Google Drive - Virus scan warning</title><meta http-equiv=\"content-type\" content=\"text/html; charset=utf-8\"/><style nonce=\"QQb_p0HGjtdFFmG9hGyzJw\">.goog-link-button{position:relative;color:#15c;text-decoration:underline;cursor:pointer}.goog-link-button-disabled{color:#ccc;text-decoration:none;cursor:default}body{color:#222;font:normal 13px/1.4 arial,sans-serif;margin:0}.grecaptcha-badge{visibility:hidden}.uc-main{padding-top:50px;text-align:center}#uc-dl-icon{display:inline-block;margin-top:16px;padding-right:1em;vertical-align:top}#uc-text{display:inline-block;max-width:68ex;text-align:left}.uc-error-caption,.uc-warning-caption{color:#222;font-size:16px}#uc-download-link{text-decoration:none}.uc-name-size a{color:#15c;text-decoration:none}.uc-name-size a:visited{color:#61c;text-decoration:none}.uc-name-size a:active{color:#d14836;text-decoration:none}.uc-footer{color:#777;font-size:11px;padding-bottom:5ex;padding-top:5ex;text-align:center}.uc-footer a{color:#15c}.uc-footer a:visited{color:#61c}.uc-footer a:active{color:#d14836}.uc-footer-divider{color:#ccc;width:100%}.goog-inline-block{position:relative;display:-moz-inline-box;display:inline-block}* html .goog-inline-block{display:inline}*:first-child+html .goog-inline-block{display:inline}sentinel{}</style><link rel=\"icon\" href=\"//ssl.gstatic.com/docs/doclist/images/drive_2022q3_32dp.png\"/></head><body><div class=\"uc-main\"><div id=\"uc-dl-icon\" class=\"image-container\"><div class=\"drive-sprite-aux-download-file\"></div></div><div id=\"uc-text\"><p class=\"uc-warning-caption\">Google Drive can't scan this file for viruses.</p><p class=\"uc-warning-subcaption\"><span class=\"uc-name-size\"><a href=\"/open?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM\">img_align_celeba.zip</a> (1.3G)</span> is too large for Google to scan for viruses. Would you still like to download this file?</p><form id=\"download-form\" action=\"https://drive.usercontent.google.com/download\" method=\"get\"><input type=\"submit\" id=\"uc-download-link\" class=\"goog-inline-block jfk-button jfk-button-action\" value=\"Download anyway\"/><input type=\"hidden\" name=\"id\" value=\"0B7EVK8r0v71pZjFTYXZWM3FlRnM\"><input type=\"hidden\" name=\"export\" value=\"download\"><input type=\"hidden\" name=\"confirm\" value=\"t\"><input type=\"hidden\" name=\"uuid\" value=\"532f92a3-b15d-4741-acb7-6e471dfa4c04\"></form></div></div><div class=\"uc-footer\"><hr class=\"uc-footer-divider\"></div></body></html>\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The MD5 checksum of the download file D:/downloads/btp/outputforpaper/CELEBA/celeba/img_align_celeba.zip does not match the one on record.Please delete the file and try again. If the issue persists, please report this to torchvision at https://github.com/pytorch/vision/issues.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train_dataset = CelebA('D:/downloads/btp/outputforpaper/CELEBA', split=\"train\", download=False, transform=transforms.ToTensor())\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# test_dataset = CelebA('D:/downloads/btp/outputforpaper/CELEBA', split=\"test\", download=False, transform=transforms.ToTensor())\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCelebA\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD:/downloads/btp/outputforpaper/CELEBA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m CelebA(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/downloads/btp/outputforpaper/CELEBA\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mToTensor())\n",
      "File \u001b[0;32m~/miniconda3/envs/nitinmtp/lib/python3.9/site-packages/torchvision/datasets/celeba.py:80\u001b[0m, in \u001b[0;36mCelebA.__init__\u001b[0;34m(self, root, split, target_type, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_transform is specified but target_type is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_integrity():\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nitinmtp/lib/python3.9/site-packages/torchvision/datasets/celeba.py:150\u001b[0m, in \u001b[0;36mCelebA.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (file_id, md5, filename) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_list:\n\u001b[0;32m--> 150\u001b[0m     \u001b[43mdownload_file_from_google_drive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_folder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m extract_archive(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg_align_celeba.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/nitinmtp/lib/python3.9/site-packages/torchvision/datasets/utils.py:268\u001b[0m, in \u001b[0;36mdownload_file_from_google_drive\u001b[0;34m(file_id, root, filename, md5)\u001b[0m\n\u001b[1;32m    260\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    261\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe detected some HTML elements in the downloaded file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis most likely means that the download triggered an unhandled API response by GDrive. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease report this to torchvision at https://github.com/pytorch/vision/issues including \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m             )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m md5 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_md5(fpath, md5):\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    269\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe MD5 checksum of the download file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match the one on record.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease delete the file and try again. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf the issue persists, please report this to torchvision at https://github.com/pytorch/vision/issues.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The MD5 checksum of the download file D:/downloads/btp/outputforpaper/CELEBA/celeba/img_align_celeba.zip does not match the one on record.Please delete the file and try again. If the issue persists, please report this to torchvision at https://github.com/pytorch/vision/issues."
     ]
    }
   ],
   "source": [
    "# train_dataset = CelebA('D:/downloads/btp/outputforpaper/CELEBA', split=\"train\", download=False, transform=transforms.ToTensor())\n",
    "# test_dataset = CelebA('D:/downloads/btp/outputforpaper/CELEBA', split=\"test\", download=False, transform=transforms.ToTensor())\n",
    "train_dataset = CelebA('D:/downloads/btp/outputforpaper/CELEBA', split=\"train\", download=True, transform=transforms.ToTensor())\n",
    "test_dataset = CelebA('D:/downloads/btp/outputforpaper/CELEBA', split=\"test\", download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_size = len(train_dataset)\n",
    "total_test_size = len(test_dataset)\n",
    "input_dim = 12288\n",
    "num_clients = 50\n",
    "rounds = 50\n",
    "batch_size = 16\n",
    "in_group_rounds = 5 \n",
    "epochs_per_client = 5\n",
    "learning_rate = 5e-3 \n",
    "num_malicious_clients = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "def average_models(new_model, next, scale):\n",
    "    if new_model == None:\n",
    "        new_model = next\n",
    "        for key in new_model:\n",
    "            new_model[key] = new_model[key] * scale\n",
    "    else:\n",
    "        for key in new_model:\n",
    "            new_model[key] = new_model[key] + (next[key] * scale)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedNet(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # First convolution block (64x64 → 32x32)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # Use 3x3 kernel\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),  # LeakyReLU instead of ReLU\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),  # Extra conv layer\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2)  # Downsample 64x64 → 32x32\n",
    "        )\n",
    "        \n",
    "        # Second convolution block (32x32 → 16x16)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # Extra conv layer\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2)  # Downsample 32x32 → 16x16\n",
    "        )\n",
    "\n",
    "        # Third convolution block (16x16 → 8x8)\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),  # Extra conv layer\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2)  # Downsample 16x16 → 8x8\n",
    "        )\n",
    "\n",
    "        # Fourth convolution block (8x8 → 4x4)\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),  # Extra conv layer\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2)  # Downsample 8x8 → 4x4\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling + Fully Connected Layer\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)  # Reduces 4x4x256 → 1x1x256\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),  # Reduce to 128 features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),  # Dropout to prevent overfitting\n",
    "            nn.Linear(128, 40)  # Output 40 attributes for CelebA\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.gap(out)  # Global Average Pooling\n",
    "        out = self.fc(out)\n",
    "        return out  # No sigmoid (BCEWithLogitsLoss applies it internally)\n",
    "    \n",
    "    def batch_accuracy(self, outputs, labels):\n",
    "        with torch.no_grad():\n",
    "            predictions = torch.sigmoid(outputs) > 0.5  # Convert logits to binary predictions\n",
    "            correct = (predictions == labels).float().mean()  # Average over all attributes\n",
    "            return correct\n",
    "    \n",
    "    def _process_batch(self, batch):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = nn.BCEWithLogitsLoss()(outputs, labels.float())  # Use BCE loss for multi-label classification\n",
    "        accuracy = self.batch_accuracy(outputs, labels)\n",
    "        return loss, accuracy\n",
    "    \n",
    "    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.Adam):\n",
    "        dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "        optimizer = opt(self.parameters(), lr)\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            accs = []\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss.detach()\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "            avg_loss = torch.stack(losses).mean().item()\n",
    "            avg_acc = torch.stack(accs).mean().item()\n",
    "            history.append((avg_loss, avg_acc))\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, dataset, batch_size=128):\n",
    "        dataloader = DataLoader(dataset, batch_size)\n",
    "        losses = []\n",
    "        accs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "        avg_loss = torch.stack(losses).mean().item()\n",
    "        avg_acc = torch.stack(accs).mean().item()\n",
    "        return (avg_loss, avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data_label_quantity(\n",
    "    num_clients, labels_per_client, train_dataset,total_train_size,seed=42):\n",
    "    trainset=train_dataset\n",
    "    prng = np.random.default_rng(seed)\n",
    "\n",
    "    targets = trainset.targets\n",
    "    if isinstance(targets, list):\n",
    "        targets = np.array(targets)\n",
    "    if isinstance(targets, torch.Tensor):\n",
    "        targets = targets.numpy()\n",
    "    num_classes = len(set(targets))\n",
    "    times = [0 for _ in range(num_classes)]\n",
    "    contains = []\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        current = [i % num_classes]\n",
    "        times[i % num_classes] += 1\n",
    "        j = 1\n",
    "        while j < labels_per_client:\n",
    "            index = prng.choice(num_classes, 1)[0]\n",
    "            if index not in current:\n",
    "                current.append(index)\n",
    "                times[index] += 1\n",
    "                j += 1\n",
    "        contains.append(current)\n",
    "    idx_clients: List[List] = [[] for _ in range(num_clients)]\n",
    "    for i in range(num_classes):\n",
    "        idx_k = np.where(targets == i)[0]\n",
    "        prng.shuffle(idx_k)\n",
    "        idx_k_split = np.array_split(idx_k, times[i])\n",
    "        ids = 0\n",
    "        for j in range(num_clients):\n",
    "            if i in contains[j]:\n",
    "                idx_clients[j] += idx_k_split[ids].tolist()\n",
    "                ids += 1\n",
    "    trainsets_per_client = [Subset(trainset, idxs) for idxs in idx_clients]\n",
    "    return trainsets_per_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_datasets = partition_data_label_quantity(num_clients,2,train_dataset,len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class memory:\n",
    "\n",
    "    def __init__(self, batch_number, batch_size):\n",
    "        self.batch_number = batch_number\n",
    "        self.batch_size = batch_size\n",
    "        self.matrix = np.zeros((batch_number, batch_size), dtype=int)\n",
    "        self.add_time = 0\n",
    "\n",
    "    def add_number(self, batch_no, true_array):\n",
    "        if batch_no >= self.batch_number:\n",
    "            print(f\"Warning: Batch number {batch_no} out of range. Skipping this entry.\")\n",
    "            return\n",
    "        # Ensure true_array length matches batch_size by padding or truncating as necessary\n",
    "        true_array = np.pad(true_array, (0, max(0, self.batch_size - len(true_array))), mode='constant')[:self.batch_size]\n",
    "        self.matrix[batch_no] += true_array\n",
    "\n",
    "\n",
    "    def add(self):\n",
    "        self.add_time = self.add_time + 1\n",
    "\n",
    "    def get_ambiguous(self, number):\n",
    "        temp_matrix = self.matrix\n",
    "        temp_matrix = np.abs(temp_matrix - np.ones(self.matrix.shape) * self.add_time * 0.5)\n",
    "        temp_array = np.reshape(temp_matrix, self.matrix.shape[0] * self.matrix.shape[1])\n",
    "        temp = list(map(list, zip(range(len(temp_array)), temp_array)))\n",
    "        small = sorted(temp, key=lambda x: x[1], reverse=False)\n",
    "        small_array = []\n",
    "        for i in range(number):\n",
    "            small_array.append(small[i][0])\n",
    "        return small_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class KMT:\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.5\n",
    "    batch_size = 32\n",
    "    batch_number = 1000\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    def __init__(self, dataset, index, test_loader):\n",
    "        self.load_train_set(dataset, index)\n",
    "        self.test_loader = test_loader\n",
    "        self.model = FederatedNet()  # Using CelebA-compatible model\n",
    "        self.criterion = nn.BCEWithLogitsLoss()  # Change for multi-label classification\n",
    "        self.distillation_criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.learning_rate, momentum=self.momentum)\n",
    "\n",
    "    def load_train_set(self, dataset, index):\n",
    "        train_dataset = torch.utils.data.Subset(dataset, index.astype(int))\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def train(self):\n",
    "        running_loss = 0\n",
    "        batch_number = 0\n",
    "\n",
    "        for images, labels in self.train_loader:\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(images)\n",
    "            loss = self.criterion(output, labels.float())  # Ensure labels are float for BCE\n",
    "\n",
    "            # Multi-label accuracy computation\n",
    "            predictions = (torch.sigmoid(output) > 0.5).int()\n",
    "            equals = (predictions == labels).cpu().numpy().astype(int)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            batch_number += 1\n",
    "\n",
    "        self.train_loss.append(running_loss / len(self.train_loader))\n",
    "\n",
    "    def distillation_train(self, images, labels, other_soft_decision):\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(images)\n",
    "        loss1 = self.criterion(output, labels.float())\n",
    "        loss2 = self.distillation_criterion(torch.sigmoid(output), other_soft_decision)\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        predictions = (torch.sigmoid(output) > 0.5).int()\n",
    "        equals = (predictions == labels).cpu().numpy().astype(int)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return equals\n",
    "\n",
    "    def test(self):\n",
    "        accuracy = 0\n",
    "        loss = 0\n",
    "        for images, labels in self.test_loader:\n",
    "            output = self.model(images)\n",
    "            loss += self.criterion(output, labels.float())\n",
    "\n",
    "            predictions = (torch.sigmoid(output) > 0.5).int()\n",
    "            equals = (predictions == labels).float()\n",
    "            accuracy += torch.mean(equals)\n",
    "\n",
    "        self.test_loss.append((loss / len(self.test_loader)).item())\n",
    "        self.test_accuracy.append((accuracy / len(self.test_loader)).item())\n",
    "        return accuracy / len(self.test_loader)\n",
    "\n",
    "    def get_ambiguous_data(self, array):\n",
    "        np.sort(array)\n",
    "        batch_number = 0\n",
    "        ambiguous_images = []\n",
    "        ambiguous_labels = []\n",
    "        for images, labels in self.train_loader:\n",
    "            for no in array:\n",
    "                if int(no / self.batch_size) == batch_number:\n",
    "                    column = np.mod(no, self.batch_size)\n",
    "                    ambiguous_images.append(images[column])\n",
    "                    ambiguous_labels.append(labels[column])\n",
    "            batch_number += 1\n",
    "        ambiguous_images = torch.stack(ambiguous_images)\n",
    "        ambiguous_labels = torch.stack(ambiguous_labels)\n",
    "        return ambiguous_images, ambiguous_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, dataset, test_loader):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "        # self.test_loader = test_loader\n",
    "        self.kmt = KMT(dataset, index=np.arange(len(dataset)), test_loader=test_loader)\n",
    "        self.client_data = []\n",
    "\n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "\n",
    "    def train(self, parameters_dict, other_soft_decisions=None):\n",
    "        # Load initial parameters\n",
    "        self.kmt.model.load_state_dict(parameters_dict)\n",
    "\n",
    "        # Standard training\n",
    "        self.kmt.train()\n",
    "\n",
    "        # Knowledge distillation using soft labels from other clients (if available)\n",
    "        if other_soft_decisions is not None:\n",
    "            for images, labels in self.kmt.train_loader:\n",
    "                if images in other_soft_decisions:\n",
    "                    other_soft_decision = other_soft_decisions[images]\n",
    "                    self.kmt.distillation_train(images, labels, other_soft_decision)\n",
    "        \n",
    "        # Return the updated model parameters\n",
    "        return self.kmt.model.state_dict()\n",
    "\n",
    "    def recieve_parameters(self, client_id, client_parameter, fraction):\n",
    "        client_object = {\"client_id\": client_id, \"client_parameter\": client_parameter, \"fraction\": fraction}\n",
    "        self.client_data.append(client_object)\n",
    "\n",
    "    def aggregated_parameters(self):\n",
    "        local_aggregated_model = None\n",
    "        for i in range(len(self.client_data)):\n",
    "            local_aggregated_model = average_models(local_aggregated_model, self.client_data[i][\"client_parameter\"], self.client_data[i][\"fraction\"])\n",
    "        self.client_data.clear()\n",
    "        return local_aggregated_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=200, shuffle=True)\n",
    "clients = [Client(i, client_datasets[i], test_loader) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl_start_time = time.time()\n",
    "\n",
    "global_net = to_device(FederatedNet(), device)\n",
    "curr_parameters = global_net.state_dict()\n",
    "history_dfl = []\n",
    "weights = []\n",
    "traffic = []\n",
    "load_model = None\n",
    "total_data_size = len(train_dataset)\n",
    "\n",
    "for i in range(rounds):\n",
    "    new_model = None\n",
    "    soft_decisions = {}  # Dictionary to store each client's soft decisions for distillation\n",
    "\n",
    "    # Step 1: Each client performs training and generates soft decisions\n",
    "    for client in clients:\n",
    "        fraction = client.get_dataset_size() / total_data_size\n",
    "\n",
    "        # Each client trains on its dataset, possibly using other clients' soft decisions for distillation\n",
    "        client_parameters = client.train(curr_parameters, other_soft_decisions=soft_decisions if i > 0 else None)\n",
    "        \n",
    "        # Gather soft decisions for this client's data to use in distillation\n",
    "        for images, labels in client.kmt.train_loader:\n",
    "            output = client.kmt.model(images)\n",
    "            soft_decisions[images] = torch.softmax(output, dim=1).detach()\n",
    "\n",
    "        # Share parameters with other clients for aggregation\n",
    "        for clt in clients:\n",
    "            clt.recieve_parameters(client.get_client_id(), client_parameters, fraction)\n",
    "\n",
    "        # Aggregate models across all clients\n",
    "        new_model = average_models(new_model, client_parameters, fraction)\n",
    "\n",
    "    # Update the global model and evaluate\n",
    "    load_model = new_model.copy()\n",
    "    global_net.load_state_dict(new_model)\n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    dev_loss, dev_acc = global_net.evaluate(test_dataset)\n",
    "    \n",
    "    # Logging\n",
    "    print(f'After round {i + 1}, train_loss = {round(train_loss, 4)}, dev_loss = {round(dev_loss, 4)}, dev_acc = {round(dev_acc, 4)}')\n",
    "    history_dfl.append((train_acc, dev_acc, i))\n",
    "    traffic.append((num_clients) * (num_clients - 1))\n",
    "\n",
    "# Calculate total time\n",
    "dfl_end_time = time.time()\n",
    "dfl_time = dfl_end_time - dfl_start_time\n",
    "print(f\"Total training time: {dfl_time:.2f} seconds\")\n",
    "\n",
    "# Optionally save the final global model if needed\n",
    "torch.save(global_net.state_dict(), \"final_global_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history_dfl))], [history_dfl[i][0] for i in range(len(history_dfl))], color='r', label='train accuracy')\n",
    "plt.plot([i + 1 for i in range(len(history_dfl))], [history_dfl[i][1] for i in range(len(history_dfl))], color='b', label='dev accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Rounds vs Accuracy\")\n",
    "plt.xlabel(\"Number of rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, dataset, test_loader):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "        self.kmt = KMT(dataset, index=np.arange(len(dataset)), test_loader=test_loader)\n",
    "        self.client_data=[]\n",
    "        self.server_id=-1\n",
    "        self.distance=random.randint(1,100)\n",
    "\n",
    "    def set_server_id(self,client_id):\n",
    "        self.client_data_server=[]\n",
    "        self.server_id=client_id\n",
    "    \n",
    "    def get_server_id(self):\n",
    "        return self.server_id\n",
    "\n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "\n",
    "    def train(self, parameters_dict, other_soft_decisions=None):\n",
    "        # Load initial parameters\n",
    "        self.kmt.model.load_state_dict(parameters_dict)\n",
    "\n",
    "        # Standard training\n",
    "        self.kmt.train()\n",
    "\n",
    "        # Knowledge distillation using soft labels from other clients (if available)\n",
    "        if other_soft_decisions is not None:\n",
    "            for images, labels in self.kmt.train_loader:\n",
    "                if images in other_soft_decisions:\n",
    "                    other_soft_decision = other_soft_decisions[images]\n",
    "                    self.kmt.distillation_train(images, labels, other_soft_decision)\n",
    "        \n",
    "        # Return the updated model parameters\n",
    "        return self.kmt.model.state_dict()\n",
    "\n",
    "    def recieve_parameters(self, client_id, client_parameter, fraction):\n",
    "        client_object = {\"client_id\": client_id, \"client_parameter\": client_parameter, \"fraction\": fraction}\n",
    "        self.client_data.append(client_object)\n",
    "\n",
    "    def aggregated_parameters(self):\n",
    "        local_aggregated_model = None\n",
    "        for i in range(len(self.client_data)):\n",
    "            local_aggregated_model = average_models(local_aggregated_model, self.client_data[i][\"client_parameter\"], self.client_data[i][\"fraction\"])\n",
    "        self.client_data.clear()\n",
    "        return local_aggregated_model\n",
    "    \n",
    "    def recieve_data_as_server(self,client_parameter,fraction):\n",
    "        client_object={\"client_parameter\":client_parameter,\"fraction\":fraction}\n",
    "        self.client_data_server.append(client_object)\n",
    "    \n",
    "\n",
    "    def aggregate_parameter_as_server(self):\n",
    "         local_aggregated_model_as_server=None\n",
    "         for data in self.client_data_server:\n",
    "              local_aggregated_model_as_server=average_models(local_aggregated_model_as_server,data[\"client_parameter\"],data[\"fraction\"])\n",
    "         self.client_data_server.clear()\n",
    "         return local_aggregated_model_as_server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_function(client_set_1,client_set_2,data):\n",
    "  client_set_1_average=None\n",
    "  client_set_2_average=None\n",
    "  for i in client_set_1:\n",
    "    client_set_1_average=average_models(client_set_1_average,data[i],1)\n",
    "  for i in client_set_2:\n",
    "    client_set_2_average=average_models(client_set_2_average,data[i],1)\n",
    "  model1_weights = []\n",
    "  model2_weights = []\n",
    "  for key, value in client_set_1_average.items():\n",
    "      model1_weights.append(value.detach().cpu())\n",
    "\n",
    "  for key, value in client_set_2_average.items():\n",
    "      model2_weights.append(value.detach().cpu())\n",
    "  weight1_flat = torch.cat([param.view(-1) for param in model1_weights])\n",
    "  weight2_flat = torch.cat([param.view(-1) for param in model2_weights])\n",
    "  cosine_sim = torch.nn.functional.cosine_similarity(weight1_flat, weight2_flat, dim=0)\n",
    "  dissimilarity_score = 1 - cosine_sim.item()\n",
    "  return dissimilarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateGroupsGame(data,num_clients):\n",
    "    groups=[]\n",
    "    for i in range(0,num_clients):\n",
    "        groups.append([i])\n",
    "    while True:\n",
    "        for group in groups:\n",
    "            for x in group:\n",
    "                no_change_flag=0\n",
    "                max_group=None\n",
    "                max_utility=0\n",
    "                for g in groups:\n",
    "                    if g!=group:\n",
    "                        if len(g) >= 1 and len(group) > 1:\n",
    "                            tempset_1=g.copy()\n",
    "                            tempset_1.append(x)\n",
    "                            tempset_2=group.copy()\n",
    "                            tempset_2.remove(x)\n",
    "                            if utility_function(tempset_1,tempset_2,data) > utility_function(g,group,data): \n",
    "                                if max_utility < utility_function(tempset_1,tempset_2,data):\n",
    "                                    max_group=g\n",
    "                                    max_utility=utility_function(tempset_1,tempset_2,data)        \n",
    "                        elif len(group)<=1:\n",
    "                            if utility_function(g,group,data) > max_utility:\n",
    "                                max_group=g\n",
    "                                max_utility=utility_function(g,group,data)\n",
    "                if max_group is not None:  \n",
    "                    group.remove(x)\n",
    "                    max_group.append(x)\n",
    "                    no_change_flag=1\n",
    "                    if len(group) <=0 :\n",
    "                        groups.remove(group)\n",
    "        if no_change_flag == 0:\n",
    "            break\n",
    "    print(\"end\")\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupDataLength(groups,clients):\n",
    "    group_dataset_length=[]\n",
    "    for grp in groups:\n",
    "        totalcount=0\n",
    "        for clt in grp:\n",
    "            totalcount+=clients[clt].get_dataset_size()\n",
    "        group_dataset_length.append(totalcount)\n",
    "    return group_dataset_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=200, shuffle=True)\n",
    "clients = [Client(i, client_datasets[i], test_loader) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_net = to_device(FederatedNet(), device)\n",
    "curr_parameters = global_net.state_dict()\n",
    "data = []\n",
    "for client in clients:\n",
    "    paramerter=client.train(curr_parameters)\n",
    "    data.append(paramerter)\n",
    "\n",
    "groups=generateGroupsGame(data,num_clients)\n",
    "print(groups)\n",
    "group_dataset_length=groupDataLength(groups,clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfl_start_time = time.time()\n",
    "\n",
    "global_net = to_device(FederatedNet(), device)\n",
    "curr_parameters = global_net.state_dict()\n",
    "history_hdfl = []\n",
    "weights = []\n",
    "traffic = []\n",
    "load_model = None\n",
    "total_data_size = len(train_dataset)\n",
    "\n",
    "for i in range(rounds):\n",
    "    new_model = None\n",
    "    soft_decisions = {}  # Dictionary to store each client's soft decisions for distillation\n",
    "\n",
    "    # Step 1: Each client performs training and generates soft decisions\n",
    "    for client in clients:\n",
    "        fraction = client.get_dataset_size() / total_data_size\n",
    "\n",
    "        # Each client trains on its dataset, possibly using other clients' soft decisions for distillation\n",
    "        client_parameters = client.train(curr_parameters, other_soft_decisions=soft_decisions if i > 0 else None)\n",
    "        \n",
    "        # Gather soft decisions for this client's data to use in distillation\n",
    "        for images, labels in client.kmt.train_loader:\n",
    "            output = client.kmt.model(images)\n",
    "            soft_decisions[images] = torch.softmax(output, dim=1).detach()\n",
    "\n",
    "        # Share parameters with other clients for aggregation\n",
    "        for clt in clients:\n",
    "            clt.recieve_parameters(client.get_client_id(), client_parameters, fraction)\n",
    "\n",
    "        # Aggregate models across all clients\n",
    "        new_model = average_models(new_model, client_parameters, fraction)\n",
    "\n",
    "    # Update the global model and evaluate\n",
    "    load_model = new_model.copy()\n",
    "    global_net.load_state_dict(new_model)\n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    dev_loss, dev_acc = global_net.evaluate(test_dataset)\n",
    "    \n",
    "    # Logging\n",
    "    print(f'After round {i + 1}, train_loss = {round(train_loss, 4)}, dev_loss = {round(dev_loss, 4)}, dev_acc = {round(dev_acc, 4)}')\n",
    "    history_hdfl.append((train_acc, dev_acc, i))\n",
    "    traffic.append((num_clients) * (num_clients - 1))\n",
    "\n",
    "# Calculate total time\n",
    "hdfl_end_time = time.time()\n",
    "hdfl_time = hdfl_end_time - hdfl_start_time\n",
    "print(f\"Total training time: {hdfl_time:.2f} seconds\")\n",
    "\n",
    "# Optionally save the final global model if needed\n",
    "torch.save(global_net.state_dict(), \"final_global_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history_hdfl))], [history_hdfl[i][0] for i in range(len(history_hdfl))], color='r', label='train accuracy')\n",
    "plt.plot([i + 1 for i in range(len(history_hdfl))], [history_hdfl[i][1] for i in range(len(history_hdfl))], color='b', label='dev accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Rounds vs Accuracy\")\n",
    "plt.xlabel(\"Number of rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history_dfl))], [history_dfl[i][1] for i in range(len(history_dfl))], color='b', label='DFL dev accuracy')\n",
    "plt.plot([i + 1 for i in range(len(history_hdfl))], [history_hdfl[i][1] for i in range(len(history_hdfl))], color='r', label='HDFL dev accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Rounds vs Accuracy on CIFAR10\")\n",
    "plt.xlabel(\"Number of rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['DFL', 'HDFL']\n",
    "values = [dfl_time, hdfl_time]\n",
    "\n",
    "# Plot the bar graph\n",
    "plt.bar(labels, values, color=['green', 'red'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('')\n",
    "plt.ylabel('RUN TIME')\n",
    "plt.title('Run Time for DFL & HDFL')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
