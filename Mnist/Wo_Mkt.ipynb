{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import pickle\n",
    "import time\n",
    "from torch.utils.data import ConcatDataset, Dataset, Subset\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to D:/downloads/btp/outputforpaper/FashionMnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:/downloads/btp/outputforpaper/FashionMnist/FashionMNIST/raw/train-images-idx3-ubyte.gz to D:/downloads/btp/outputforpaper/FashionMnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to D:/downloads/btp/outputforpaper/FashionMnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:/downloads/btp/outputforpaper/FashionMnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz to D:/downloads/btp/outputforpaper/FashionMnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to D:/downloads/btp/outputforpaper/FashionMnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:/downloads/btp/outputforpaper/FashionMnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to D:/downloads/btp/outputforpaper/FashionMnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to D:/downloads/btp/outputforpaper/FashionMnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting D:/downloads/btp/outputforpaper/FashionMnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to D:/downloads/btp/outputforpaper/FashionMnist/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FashionMNIST('D:/downloads/btp/outputforpaper/FashionMnist', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = FashionMNIST('D:/downloads/btp/outputforpaper/FashionMnist', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_size = len(train_dataset)\n",
    "total_test_size = len(test_dataset)\n",
    "classes = 10\n",
    "input_dim = 784\n",
    "num_clients = 25\n",
    "rounds = 50\n",
    "batch_size = 32\n",
    "in_group_rounds=5\n",
    "epochs_per_client =5\n",
    "learning_rate = 2e-2\n",
    "num_malicious_clients=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_train_size, total_test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader(DataLoader):\n",
    "        def __init__(self, dl, device):\n",
    "            self.dl = dl\n",
    "            self.device = device\n",
    "\n",
    "        def __iter__(self):\n",
    "            for batch in self.dl:\n",
    "                yield to_device(batch, self.device)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dl)\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "def average_models(new_model, next, scale):\n",
    "    if new_model == None:\n",
    "        new_model = next\n",
    "        for key in new_model:\n",
    "            new_model[key] = new_model[key] * scale\n",
    "    else:\n",
    "        for key in new_model:\n",
    "            new_model[key] = new_model[key] + (next[key] * scale)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FederatedNet(torch.nn.Module):    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 20, 7)\n",
    "        self.conv2 = torch.nn.Conv2d(20, 40, 7)\n",
    "        self.maxpool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear = torch.nn.Linear(2560, 10)\n",
    "        self.non_linearity = torch.nn.functional.relu\n",
    "    \n",
    "    def forward(self, x_batch):\n",
    "        out = self.conv1(x_batch)\n",
    "        out = self.non_linearity(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.non_linearity(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "    def batch_accuracy(self, outputs, labels):\n",
    "        with torch.no_grad():\n",
    "            _, predictions = torch.max(outputs, dim=1)\n",
    "            return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
    "    \n",
    "    def _process_batch(self, batch):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        accuracy = self.batch_accuracy(outputs, labels)\n",
    "        return (loss, accuracy)\n",
    "    \n",
    "    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n",
    "        optimizer = opt(self.parameters(), lr)\n",
    "        history = []\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            accs = []\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss.detach()\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "            avg_loss = torch.stack(losses).mean().item()\n",
    "            avg_acc = torch.stack(accs).mean().item()\n",
    "            history.append((avg_loss, avg_acc))\n",
    "        return history\n",
    "    \n",
    "    def evaluate(self, dataset, batch_size=128):\n",
    "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n",
    "        losses = []\n",
    "        accs = []\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                loss, acc = self._process_batch(batch)\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "        avg_loss = torch.stack(losses).mean().item()\n",
    "        avg_acc = torch.stack(accs).mean().item()\n",
    "        return (avg_loss, avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data_label_quantity(\n",
    "    num_clients, labels_per_client, train_dataset,total_train_size,seed=42):\n",
    "    trainset=train_dataset\n",
    "    prng = np.random.default_rng(seed)\n",
    "\n",
    "    targets = trainset.targets\n",
    "    if isinstance(targets, list):\n",
    "        targets = np.array(targets)\n",
    "    if isinstance(targets, torch.Tensor):\n",
    "        targets = targets.numpy()\n",
    "    num_classes = len(set(targets))\n",
    "    times = [0 for _ in range(num_classes)]\n",
    "    contains = []\n",
    "\n",
    "    for i in range(num_clients):\n",
    "        current = [i % num_classes]\n",
    "        times[i % num_classes] += 1\n",
    "        j = 1\n",
    "        while j < labels_per_client:\n",
    "            index = prng.choice(num_classes, 1)[0]\n",
    "            if index not in current:\n",
    "                current.append(index)\n",
    "                times[index] += 1\n",
    "                j += 1\n",
    "        contains.append(current)\n",
    "    idx_clients: List[List] = [[] for _ in range(num_clients)]\n",
    "    for i in range(num_classes):\n",
    "        idx_k = np.where(targets == i)[0]\n",
    "        prng.shuffle(idx_k)\n",
    "        idx_k_split = np.array_split(idx_k, times[i])\n",
    "        ids = 0\n",
    "        for j in range(num_clients):\n",
    "            if i in contains[j]:\n",
    "                idx_clients[j] += idx_k_split[ids].tolist()\n",
    "                ids += 1\n",
    "    trainsets_per_client = [Subset(trainset, idxs) for idxs in idx_clients]\n",
    "    return trainsets_per_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_datasets = partition_data_label_quantity(num_clients,2,train_dataset,len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, dataset):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "    \n",
    "    def train(self, parameters_dict):\n",
    "        net = to_device(FederatedNet(), device)\n",
    "        net.load_state_dict(parameters_dict)\n",
    "        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n",
    "        return net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [Client('client_' + str(i), client_datasets[i]) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After round 1, train_loss = 2.1154, dev_loss = 2.1162, dev_acc = 0.2155\n",
      "\n",
      "After round 2, train_loss = 1.6971, dev_loss = 1.7018, dev_acc = 0.5615\n",
      "\n",
      "After round 3, train_loss = 1.4874, dev_loss = 1.4935, dev_acc = 0.4521\n",
      "\n",
      "After round 4, train_loss = 1.3235, dev_loss = 1.3325, dev_acc = 0.519\n",
      "\n",
      "After round 5, train_loss = 1.1411, dev_loss = 1.1525, dev_acc = 0.6225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fl_start_time = time.time()\n",
    "\n",
    "global_net = to_device(FederatedNet(), device)\n",
    "curr_parameters = global_net.state_dict()\n",
    "history_fl = []\n",
    "traffic=[]\n",
    "weights=[]\n",
    "load_model=None\n",
    "total_length_dataset=len(train_dataset)\n",
    "for i in range(rounds):\n",
    "    new_model=None\n",
    "    curr_parameters = global_net.state_dict()\n",
    "    for client in clients:\n",
    "        client_parameters = client.train(curr_parameters)\n",
    "        fraction = client.get_dataset_size()/total_length_dataset\n",
    "        new_model=average_models(new_model,client_parameters,fraction)\n",
    "    load_model=new_model.copy()\n",
    "    params = list(load_model)\n",
    "    total_norm = 0.0\n",
    "    for key, value in load_model.items():\n",
    "        total_norm += torch.norm(value.view(-1)).item()\n",
    "    weights.append(total_norm)\n",
    "    global_net.load_state_dict(new_model)\n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    dev_loss, dev_acc = global_net.evaluate(test_dataset)\n",
    "    print('After round {}, train_loss = {}, dev_loss = {}, dev_acc = {}\\n'.format(i + 1, round(train_loss, 4), \n",
    "            round(dev_loss, 4), round(dev_acc, 4)))\n",
    "    traffic.append(2*num_clients)\n",
    "    history_fl.append((train_acc, dev_acc,i))\n",
    "\n",
    "fl_end_time = time.time()\n",
    "fl_time = fl_end_time - fl_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history_fl))], [history_fl[i][0] for i in range(len(history_fl))], color='r', label='train accuracy')\n",
    "plt.plot([i + 1 for i in range(len(history_fl))], [history_fl[i][1] for i in range(len(history_fl))], color='b', label='dev accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Rounds vs Accuracy\")\n",
    "plt.xlabel(\"Number of rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(history_fl))\n",
    "print(traffic[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, dataset):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "        self.client_data=[]\n",
    "    \n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "    \n",
    "    def train(self, parameters_dict):\n",
    "        net = to_device(FederatedNet(), device)\n",
    "        net.load_state_dict(parameters_dict)\n",
    "        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n",
    "        return net.state_dict()\n",
    "    \n",
    "    def recieve_parameters(self,client_id,client_parameter,fraction):\n",
    "        client_object={\"client_id\":client_id,\"client_parameter\":client_parameter,\"fraction\":fraction}\n",
    "        self.client_data.append(client_object)\n",
    "\n",
    "    def aggregated_parameters(self):\n",
    "            local_aggregated_model=None\n",
    "            for i in range(0,len(self.client_data)):\n",
    "                 local_aggregated_model=average_models(local_aggregated_model,self.client_data[i][\"client_parameter\"],self.client_data[i][\"fraction\"])\n",
    "            self.client_data.clear()\n",
    "            return local_aggregated_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [Client(i, client_datasets[i]) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl_start_time = time.time()\n",
    "\n",
    "global_net = to_device(FederatedNet(), device)\n",
    "curr_parameters = global_net.state_dict()\n",
    "history_dfl = []\n",
    "weights=[]\n",
    "traffic=[]\n",
    "load_model=None\n",
    "total_data_size=len(train_dataset)\n",
    "for i in range(rounds):\n",
    "    new_model=None\n",
    "    for client in clients:\n",
    "        fraction = client.get_dataset_size()/total_data_size\n",
    "        if i == 0:\n",
    "            client_parameters=client.train(curr_parameters)\n",
    "        else:\n",
    "            client_parameters = client.train(client.aggregated_parameters())\n",
    "        for clt in clients:\n",
    "                clt.recieve_parameters(client.get_client_id(),client_parameters,fraction)\n",
    "        new_model=average_models(new_model,client_parameters,fraction)\n",
    "    \n",
    "\n",
    "\n",
    "    load_model=new_model.copy()\n",
    "    params = list(load_model)\n",
    "    total_norm = 0.0\n",
    "    for key, value in load_model.items():\n",
    "            total_norm += torch.norm(value.view(-1)).item()\n",
    "    weights.append(total_norm)\n",
    "    global_net.load_state_dict(new_model)\n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    dev_loss, dev_acc = global_net.evaluate(test_dataset)\n",
    "    print('After round {}, train_loss = {}, dev_loss = {}, dev_acc = {}\\n'.format(i + 1, round(train_loss, 4), \n",
    "            round(dev_loss, 4), round(dev_acc, 4)))\n",
    "    traffic.append((num_clients)*(num_clients-1))\n",
    "    history_dfl.append((train_acc, dev_acc,i))\n",
    "\n",
    "dfl_end_time = time.time()\n",
    "dfl_time  =dfl_end_time - dfl_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history_dfl))], [history_dfl[i][0] for i in range(len(history_dfl))], color='r', label='train accuracy')\n",
    "plt.plot([i + 1 for i in range(len(history_dfl))], [history_dfl[i][1] for i in range(len(history_dfl))], color='b', label='dev accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Rounds vs Accuracy\")\n",
    "plt.xlabel(\"Number of rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(history_dfl))\n",
    "print(traffic[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_train_size = len(train_dataset)\n",
    "total_test_size = len(test_dataset)\n",
    "classes = 10\n",
    "input_dim = 784\n",
    "num_clients = 25\n",
    "rounds = 45\n",
    "batch_size = 64\n",
    "in_group_rounds=5\n",
    "epochs_per_client =5\n",
    "learning_rate = 2e-2\n",
    "num_malicious_clients=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Client:\n",
    "    def __init__(self, client_id, dataset):\n",
    "        self.client_id = client_id\n",
    "        self.dataset = dataset\n",
    "        self.client_data=[]\n",
    "        self.server_id=-1\n",
    "        self.distance=random.randint(1,100)\n",
    "        self.client_data_server=[]\n",
    "    \n",
    "    def set_server_id(self,client_id):\n",
    "        self.client_data_server=[]\n",
    "        self.server_id=client_id\n",
    "    \n",
    "    def get_server_id(self):\n",
    "        return self.server_id\n",
    "    \n",
    "    def get_dataset_size(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def get_client_id(self):\n",
    "        return self.client_id\n",
    "    \n",
    "    def train(self, parameters_dict):\n",
    "        net = to_device(FederatedNet(), device)\n",
    "        net.load_state_dict(parameters_dict)\n",
    "        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n",
    "        return net.state_dict()\n",
    "    \n",
    "    def recieve_parameters(self,client_id,client_parameter,fraction):\n",
    "        client_object={\"client_id\":client_id,\"client_parameter\":client_parameter,\"fraction\":fraction}\n",
    "        self.client_data.append(client_object)\n",
    "\n",
    "    def aggregated_parameters(self):\n",
    "            local_aggregated_model=None\n",
    "            for i in range(0,len(self.client_data)):\n",
    "                 local_aggregated_model=average_models(local_aggregated_model,self.client_data[i][\"client_parameter\"],self.client_data[i][\"fraction\"])\n",
    "            self.client_data.clear()\n",
    "            return local_aggregated_model\n",
    "    \n",
    "    def recieve_data_as_server(self,client_parameter,fraction):\n",
    "        client_object={\"client_parameter\":client_parameter,\"fraction\":fraction}\n",
    "        self.client_data_server.append(client_object)\n",
    "    \n",
    "\n",
    "    def aggregate_parameter_as_server(self):\n",
    "         local_aggregated_model_as_server=None\n",
    "         for data in self.client_data_server:\n",
    "              local_aggregated_model_as_server=average_models(local_aggregated_model_as_server,data[\"client_parameter\"],data[\"fraction\"])\n",
    "         self.client_data_server.clear()\n",
    "         return local_aggregated_model_as_server   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_function(client_set_1,client_set_2,data):\n",
    "  client_set_1_average=None\n",
    "  client_set_2_average=None\n",
    "  for i in client_set_1:\n",
    "    client_set_1_average=average_models(client_set_1_average,data[i],1)\n",
    "  for i in client_set_2:\n",
    "    client_set_2_average=average_models(client_set_2_average,data[i],1)\n",
    "  model1_weights = []\n",
    "  model2_weights = []\n",
    "  for key, value in client_set_1_average.items():\n",
    "      model1_weights.append(value.detach().cpu())\n",
    "\n",
    "  for key, value in client_set_2_average.items():\n",
    "      model2_weights.append(value.detach().cpu())\n",
    "  weight1_flat = torch.cat([param.view(-1) for param in model1_weights])\n",
    "  weight2_flat = torch.cat([param.view(-1) for param in model2_weights])\n",
    "  cosine_sim = torch.nn.functional.cosine_similarity(weight1_flat, weight2_flat, dim=0)\n",
    "  return cosine_sim.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateGroupsGame(data,num_clients):\n",
    "    groups=[]\n",
    "    for i in range(0,num_clients):\n",
    "        groups.append([i])\n",
    "    while True:\n",
    "        for group in groups:\n",
    "            for x in group:\n",
    "                no_change_flag=0\n",
    "                m_i_n_group=None\n",
    "                m_i_n_utility=100\n",
    "                minu=0\n",
    "                for g in groups:\n",
    "                    if g!=group:\n",
    "                        if len(g) >= 1 and len(group) > 1:\n",
    "                            tempset_1=g.copy()\n",
    "                            tempset_1.append(x)\n",
    "                            tempset_2=group.copy()\n",
    "                            tempset_2.remove(x)\n",
    "                            if utility_function(tempset_1,tempset_2,data) > utility_function(g,group,data): \n",
    "                                if minu < utility_function(tempset_1,tempset_2,data):\n",
    "                                    m_i_n_group=g\n",
    "                                    minu=utility_function(tempset_1,tempset_2,data)        \n",
    "                        elif len(group)<=1:\n",
    "                            if utility_function(g,group,data) < m_i_n_utility:\n",
    "                                m_i_n_group=g\n",
    "                                m_i_n_utility=utility_function(g,group,data)\n",
    "                if m_i_n_group is not None:  \n",
    "                    group.remove(x)\n",
    "                    m_i_n_group.append(x)\n",
    "                    no_change_flag=1\n",
    "                    if len(group) <=0 :\n",
    "                        groups.remove(group)\n",
    "        if no_change_flag == 0:\n",
    "            break\n",
    "    print(\"end\")\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupDataLength(groups,clients):\n",
    "    group_dataset_length=[]\n",
    "    for grp in groups:\n",
    "        totalcount=0\n",
    "        for clt in grp:\n",
    "            totalcount+=clients[clt].get_dataset_size()\n",
    "        group_dataset_length.append(totalcount)\n",
    "    return group_dataset_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clients = [Client(i, client_datasets[i]) for i in range(num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_net = to_device(FederatedNet(), device)\n",
    "curr_parameters = global_net.state_dict()\n",
    "data = []\n",
    "for client in clients:\n",
    "    paramerter=client.train(curr_parameters)\n",
    "    data.append(paramerter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=generateGroupsGame(data,num_clients)\n",
    "print(groups)\n",
    "group_dataset_length=groupDataLength(groups,clients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfl_start_time = time.time()\n",
    "\n",
    "global_net = to_device(FederatedNet(), device)\n",
    "curr_parameters = global_net.state_dict()\n",
    "history_hdfl = []\n",
    "weights=[]\n",
    "traffic=[]\n",
    "load_model=None\n",
    "total_dataset_length=len(train_dataset)\n",
    "flag=True\n",
    "for i in range(rounds):\n",
    "    traffic_single_sum=0\n",
    "    #chosing the server client with a uniform probability\n",
    "    server=[]\n",
    "    for grp in groups:\n",
    "        traffic_single_sum=traffic_single_sum+len(grp)-1\n",
    "        server_id=np.random.choice(grp,p=np.ones(len(grp))/len(grp))\n",
    "        server.append(server_id)\n",
    "        clients[server_id].set_server_id(server_id)\n",
    "    #in group mesh decentralized learning\n",
    "    k=0\n",
    "    for grp in groups:\n",
    "        flag=True\n",
    "        for j in range(in_group_rounds):\n",
    "            traffic_single_sum=traffic_single_sum+((len(grp)-1)*len(grp))\n",
    "            for clt in grp:\n",
    "                fraction = clients[clt].get_dataset_size()/group_dataset_length[k]\n",
    "                if i == 0 and flag :\n",
    "                    client_parameters=clients[clt].train(curr_parameters)\n",
    "                elif j == 0:\n",
    "                    client_parameters=clients[clt].train(load_model)\n",
    "                else:\n",
    "                    client_parameters = clients[clt].train(clients[clt].aggregated_parameters())\n",
    "                for clt_1 in grp:\n",
    "                        clients[clt_1].recieve_parameters(clt_1,client_parameters,fraction)\n",
    "            flag=False\n",
    "        #aggregating on the random choosn server\n",
    "        for clt in grp:\n",
    "                fraction = clients[clt].get_dataset_size()/group_dataset_length[k]\n",
    "                t=clients[clt].aggregated_parameters()\n",
    "                clients[server[k]].recieve_data_as_server(t,fraction)\n",
    "        k+=1\n",
    "    #aggregating the parameters\n",
    "    aggregateds=[]\n",
    "    for clt in server:\n",
    "            t=clients[clt].aggregate_parameter_as_server()\n",
    "            aggregateds.append(t)\n",
    "    #choosing the server from the amongst server at unifrom probability\n",
    "    server_id=np.random.choice(server,p=np.ones(len(server))/len(server))\n",
    "    clients[server_id].set_server_id(server_id)\n",
    "    traffic_single_sum=traffic_single_sum+len(server)\n",
    "    #sending the data to the chosen from all the group server\n",
    "    group_number=0\n",
    "    for clt in aggregateds:\n",
    "            fraction=group_dataset_length[group_number]/total_dataset_length\n",
    "            group_number+=1\n",
    "            clients[server_id].recieve_data_as_server(clt,fraction)\n",
    "    #aggregating at the top level server\n",
    "    traffic_single_sum=traffic_single_sum+num_clients        \n",
    "    aggregated_parameters_top=clients[server_id].aggregate_parameter_as_server()\n",
    "    load_model=aggregated_parameters_top.copy()\n",
    "    total_norm = 0.0\n",
    "    for key, value in load_model.items():\n",
    "            total_norm += torch.norm(value.view(-1)).item()\n",
    "    weights.append(total_norm)\n",
    "    global_net.load_state_dict(load_model)\n",
    "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
    "    dev_loss, dev_acc = global_net.evaluate(test_dataset)\n",
    "    print('After round {}, train_loss = {}, dev_loss = {}, dev_acc = {}\\n'.format(i + 1, round(train_loss, 4), \n",
    "            round(dev_loss, 4), round(dev_acc, 4)))\n",
    "    traffic.append(traffic_single_sum)\n",
    "    history_hdfl.append((train_acc, dev_acc,i))\n",
    "\n",
    "hdfl_end_time = time.time()\n",
    "hdfl_time = hdfl_end_time - hdfl_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history_hdfl))], [history_hdfl[i][0] for i in range(len(history_hdfl))], color='r', label='train accuracy')\n",
    "plt.plot([i + 1 for i in range(len(history_hdfl))], [history_hdfl[i][1] for i in range(len(history_hdfl))], color='b', label='dev accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Rounds vs Accuracy\")\n",
    "plt.xlabel(\"Number of rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(history_hdfl))\n",
    "print(traffic[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i + 1 for i in range(len(history_fl))], [history_fl[i][1] for i in range(len(history_fl))], color='b', label='FL dev accuracy')\n",
    "plt.plot([i + 1 for i in range(len(history_dfl))], [history_dfl[i][1] for i in range(len(history_dfl))], color='r', label='DFL dev accuracy')\n",
    "plt.plot([i + 1 for i in range(len(history_hdfl))], [history_hdfl[i][1] for i in range(len(history_hdfl))], color='g', label='HDFL dev accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Rounds vs Accuracy\")\n",
    "plt.xlabel(\"Number of rounds\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['FL', 'DFL', 'HDFL']\n",
    "values = [fl_time, dfl_time, hdfl_time]\n",
    "\n",
    "# Plot the bar graph\n",
    "plt.bar(labels, values, color=['blue', 'green', 'red'])\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('')\n",
    "plt.ylabel('RUN TIME')\n",
    "plt.title('Run Time for FL, DFL & HDFL')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
